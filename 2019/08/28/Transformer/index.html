<!DOCTYPE html>





<html class="theme-next pisces use-motion" lang="en">
<head>
  <meta charset="UTF-8">
<meta name="generator" content="Hexo 3.9.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/echarts.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/echarts.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"scrollpercent":true,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    save_scroll: false,
    copycode: {"enable":true,"show_result":true,"style":null},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="TransformerModel Architecture Encoder先定义符号：emb_dim：embedding的维度 input_length: 输入序列的长度（不同长度的序列通过填充来使得其长度都相同） hidden_dim:forward_feedback中隐藏层的长度 vocab_size:词汇表中单词的数量，总的数量 数据流 1.输入（input_length)  (*emb_s">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer">
<meta property="og:url" content="http://yoursite.com/2019/08/28/Transformer/index.html">
<meta property="og:site_name" content="FILE">
<meta property="og:description" content="TransformerModel Architecture Encoder先定义符号：emb_dim：embedding的维度 input_length: 输入序列的长度（不同长度的序列通过填充来使得其长度都相同） hidden_dim:forward_feedback中隐藏层的长度 vocab_size:词汇表中单词的数量，总的数量 数据流 1.输入（input_length)  (*emb_s">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/2019/08/28/Transformer/trans.png">
<meta property="og:image" content="http://yoursite.com/2019/08/28/Transformer/mult.png">
<meta property="og:image" content="http://yoursite.com/2019/08/28/Transformer/1567047917508.png">
<meta property="og:image" content="http://yoursite.com/2019/08/28/Transformer/1567048488955.png">
<meta property="og:image" content="http://yoursite.com/2019/08/28/Transformer/1567060993399.png">
<meta property="og:image" content="http://yoursite.com/2019/08/28/Transformer/11.png">
<meta property="og:image" content="http://yoursite.com/2019/08/28/Transformer/12.png">
<meta property="og:image" content="http://yoursite.com/2019/08/28/Transformer/13.png">
<meta property="og:image" content="http://yoursite.com/2019/08/28/Transformer/14.png">
<meta property="og:image" content="http://yoursite.com/2019/08/28/Transformer/15.png">
<meta property="og:image" content="http://yoursite.com/2019/08/28/Transformer/16.png">
<meta property="og:updated_time" content="2019-09-01T12:11:05.955Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Transformer">
<meta name="twitter:description" content="TransformerModel Architecture Encoder先定义符号：emb_dim：embedding的维度 input_length: 输入序列的长度（不同长度的序列通过填充来使得其长度都相同） hidden_dim:forward_feedback中隐藏层的长度 vocab_size:词汇表中单词的数量，总的数量 数据流 1.输入（input_length)  (*emb_s">
<meta name="twitter:image" content="http://yoursite.com/2019/08/28/Transformer/trans.png">
  <link rel="alternate" href="/atom.xml" title="FILE" type="application/atom+xml">
  <link rel="canonical" href="http://yoursite.com/2019/08/28/Transformer/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Transformer | FILE</title>
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  <div class="container sidebar-position-left">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">FILE</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content page-post-detail">
            

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/28/Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Les">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/echarts.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FILE">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">Transformer

            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-08-28 17:57:15" itemprop="dateCreated datePublished" datetime="2019-08-28T17:57:15+08:00">2019-08-28</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-09-01 20:11:05" itemprop="dateModified" datetime="2019-09-01T20:11:05+08:00">2019-09-01</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/ml/" itemprop="url" rel="index"><span itemprop="name">ml</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p><img src="trans.png" alt="png"></p>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><h4 id="先定义符号："><a href="#先定义符号：" class="headerlink" title="先定义符号："></a>先定义符号：</h4><p><strong>emb_dim</strong>：embedding的维度</p>
<p><strong>input_length</strong>: 输入序列的长度（不同长度的序列通过填充来使得其长度都相同）</p>
<p><strong>hidden_dim</strong>:forward_feedback中隐藏层的长度</p>
<p><strong>vocab_size</strong>:词汇表中单词的数量，总的数量</p>
<h4 id="数据流"><a href="#数据流" class="headerlink" title="数据流"></a>数据流</h4><ul>
<li><p>1.输入（<strong>input_length</strong>) <em> (<em>*emb_size</em></em>)</p>
</li>
<li><p>2.然后添加位置信息 得到（<strong>input_length</strong>) <em> (<em>*emb_size</em></em>)</p>
</li>
<li><p>3.通过N个Encoder编码块得到 （<strong>input_length</strong>) <em> (<em>*emb_size</em></em>)</p>
</li>
</ul>
<p>&emsp; <strong><em>note：</em></strong>N块Encoder的输入和输出尺寸维度是相同的，所以可以把上一个Encoder的输出，作为下一个Encoder的输入。</p>
<p>&emsp; <strong><em>note：</em></strong>每一个Encoder不会共享权重</p>
<h4 id="Input-Embedding"><a href="#Input-Embedding" class="headerlink" title="Input_Embedding"></a>Input_Embedding</h4><ul>
<li>Word_Embedding</li>
</ul>
<p>比如一句话：<em>“你好，你吃饭了吗？”</em></p>
<blockquote>
<p><strong>step1</strong> ：<em>”你好，你吃饭了吗“ -&gt; [“你好”，”,” , “你”，”吃饭”,  “了”，”吗”，”?”]</em></p>
<p><strong>step2</strong>:  <em>[“你好”，”,” , “你”，”吃饭”,  “了”，”吗”，”?”] -&gt;[2, 4, 5, 12, 35, 5, 34, 99]</em>     在语料中的index</p>
<p><strong>step3</strong>: <em>2 -&gt; E[2]  = [123.4, 0.32, ……, 32.1,32]</em></p>
</blockquote>
<p>&emsp; <strong><em>note：</em></strong>这些向量也是参数，使用反向传播进行优化/</p>
<blockquote>
<p><strong>step4</strong>：把每一个向量堆叠起来，就得到个维度为（<strong>input_length</strong>) <em> (<em>*emb_size</em></em>)的矩阵</p>
<p><strong>step5</strong>：每一句话的长度不一样，所以使用标记来填充长度，[<pad>, <pad>, “你”，”吃饭”， “了”，”吗”，”?”] </pad></pad></p>
<p>这里的<strong>input_length</strong>为7， <pad> -&gt; [999] = [213, 4.23, 413, ….]</pad></p>
</blockquote>
<ul>
<li>Positional_Encoding<br>作者是用预定的正弦函数来对位置信息进行编码<script type="math/tex; mode=display">
p_{i, j}=\left\{\begin{array}{l}{\sin \left(\frac{i}{10000^{\frac{i}{d_{e m b_{-} d i m}}}}\right)\quad if\ j\  is\  even } \\ {\cos \left(\frac{i}{10000^{\frac{i}{d_{e m b}-d i m}}}\right)\quad if\ j\  is\  odd }\end{array}\right.</script>其中<strong>i</strong>作为序列号（第几个单词）<strong>j</strong>作为embedding的位置<script type="math/tex; mode=display">
\sin \left(\frac{0}{10000^{\frac{0}{\operatorname{em} b_{d i m}}}}\right) \quad 
\cos \left(\frac{0}{10000^{\frac{0}{\operatorname{em} b_{d i m}}}}\right) \quad 
\sin \left(\frac{0}{10000^{\frac{2}{\operatorname{em} b_{d i m}}}}\right) \quad 
\cos \left(\frac{0}{10000^{\frac{2}{e m b_{d i m}}}}\right)\\
\sin \left(\frac{1}{10000^{\frac{0}{\operatorname{em} b_{d i m}}}}\right) \quad 
\cos \left(\frac{1}{10000^{\frac{0}{\operatorname{em} b_{d i m}}}}\right) \quad 
\sin \left(\frac{1}{10000^{\frac{2}{\operatorname{em} b_{d i m}}}}\right) \quad 
\cos \left(\frac{1}{10000^{\frac{2}{e m b_{d i m}}}}\right)\\\
\\
.................</script></li>
</ul>
<p>最后的结果：<strong>X = Z + P</strong></p>
<p><strong>X</strong>作为第一个Encoder的输入 （<strong>input_length</strong>) <em> (<em>*emb_size</em></em>)</p>
<h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><p><img src="mult.png" alt="png"></p>
<p><strong>（input_length）x（h * d_v）</strong>。然后，将应用具有尺寸<strong>（h * d_v）x（emb_dim）</strong>的<em>权重矩阵W’的线性层，从而导致尺寸的最终结果 <em>*（input_length）x（emb_dim）</em></em></p>
<p>Multi-Head，就是只多做几次同样的事情，同时参数不共享，然后把结果拼接</p>
<p><img src="1567047917508.png" alt="png"></p>
<script type="math/tex; mode=display">
\text { MultiHead }(Q, K, V)=\text { Concat }\left(\text { head }_{1}, \ldots, \text { head }_{\mathrm{h}}\right) W^{O}\\

\text { head }_{\mathrm{i}}=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right)</script><p>其中 Q K V 就是三个输入的矩阵，每一个头部都是由给出的三个不同 K Q V投影表示</p>
<script type="math/tex; mode=display">
\begin{array}{l}{W_{i}^{K} \text { with dimensons } d_{e m b_{-} d i m} \mathrm{x} d_{k}} \\ {W_{i}^{Q} \text { with dimensons } d_{e m b_{-} d i m} \mathrm{x} d_{k}} \\ {W_{i}^{V} \text { with dimensons } d_{e m b-d i m} \mathrm{x} d_{v}}\end{array}</script><p>输入矩阵<em>X</em>并分别将其与上述权重矩阵一起投影，得到我们的K Q V</p>
<script type="math/tex; mode=display">
\begin{aligned} X W_{i}^{K} &=K_{i} \text { with dimensons input length } \mathrm{x} d_{k} \\ X W_{i}^{Q} &=Q_{i} \text { with dimensons input length } \mathrm{x} d_{k} \\ X W_{i}^{V} &=V_{i} \text { with dimensons input length } \mathrm{x} d_{v} \end{aligned}</script><p>&emsp; <strong><em>note：</em></strong>在论文中 <strong>d_k = d_v = emb_dim / h</strong>  </p>
<p>得到K Q V 之后我们就可以使用其来计算<em>Scaled Dot-Product Attention</em>：</p>
<script type="math/tex; mode=display">
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><p><img src="1567048488955.png" alt="png"></p>
<ul>
<li><p>我们来看看 Attention 到底做了些什么</p>
<p>首先我们看 </p>
<script type="math/tex; mode=display">
Q_{i} K_{i}^{T}</script></li>
</ul>
<p>此矩阵相乘表示了什么东西？如果我们把其中的 <strong>v_i</strong> 和 <strong>u_j</strong> 单独拿出来看他们的点乘可以看作：</p>
<script type="math/tex; mode=display">
v_{i} u_{j}=\cos \left(v_{i}, u_{j}\right)\left\|v_{i}\right\|_{2}\left\|u_{j}\right\|_{2}</script><p>因此，这是对<em>u_i</em>和<em>v_j</em>的方向有多相似以及它们的长度有多大的度量（方向最接近，长度越大，点积越大）。</p>
<p>在该乘法之后，为了缩放目的，矩阵被元素划分为 <strong>d_k</strong> 的平方根。论文里对于 <strong>d_k</strong> 的作用这么来解释：<strong>d_k</strong> 很大的时候，点积得到的结果维度很大，使得结果处于 <strong>softmax</strong> 函数梯度很小的区域。这时候除以一个缩放因子，可以一定程度上减缓这种情况。</p>
<p>最后得到结果 如下：</p>
<script type="math/tex; mode=display">
\left(\begin{array}{cccccc}{72.40 * 10^{-06}} & {1.23 * 10^{-21}} & {6.51 * 10^{-40}} & {2.62 * 10^{-22}} & {9.99 * 10^{-01}} & {4.30 * 10^{-08}}  \\  
{1.00 * 10^{+00}} & {7.51 * 10^{-30}} & {1.54 * 10^{-17}} & {9.91 * 10^{-13}} & {8.15 * 10^{-69}} & {1.09 * 10^{-30}} \\ 
{3.12 * 10^{-70}} & {2.51 * 10^{-51}} & {2.72 * 10^{-21}} & {8.03 * 10^{-09}} & {1.29 * 10^{-07}} & {9.99 * 10^{-01}} \\
{2.47 * 10^{-72}} & {5.54 * 10^{-05}} & {9.80 * 10^{-01}} & {1.98 * 10^{-02}} & {2.77 * 10^{-82}} & {2.58 * 10^{-08}} \\
{2.67 * 10^{-05}} & {1.21 * 10^{-09}} & {9.75 * 10^{-07}} & {3.17 * 10^{-76}} & {9.99 * 10^{-01}} & {3.64 * 10^{-28}} \\
{8.59 * 10^{-47}} & {1.05 * 10^{-35}} & {9.99 * 10^{-01}} & {2.38 * 10^{-15}} & {4.21 * 10^{-27}} & {4.07 * 10^{-06}}\end{array}\right)</script><p>结果是数字在0和1之间的行总和为1。并且为 <strong>input_size * input_size</strong> 的大小。最后，结果乘以 <strong>V</strong>（<strong>input_size * d_v  </strong>）可以这样理解，得到的方形的矩阵，可以作为 <strong>V</strong> 的权重参数最终得到 <strong>input_size * d_v</strong>  的 <strong>head</strong> </p>
<p>此时就做到了单词和其他所有单词产生了特定的关系。 <strong>Multi-Head Attention</strong>就是做 h 次这样的处理，就会捕捉到 h 次不同的关系。然后把全部的 关系 串连到一起。</p>
<h4 id="Feed-Forward-Network"><a href="#Feed-Forward-Network" class="headerlink" title="Feed-Forward Network"></a>Feed-Forward Network</h4><p><img src="1567060993399.png" alt="png"></p>
<script type="math/tex; mode=display">
\operatorname{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}</script><p>其中 <strong>W_1 和 W_2</strong> 分别是 <strong>（emb_dim）x（d_F）</strong> 和 <strong>（d_F）x（emb_dim）</strong> 矩阵</p>
<p>最后输出为 <strong>（input_length）x（emb_dim）</strong>的矩阵</p>
<h4 id="Dropout-Add-amp-Norm"><a href="#Dropout-Add-amp-Norm" class="headerlink" title="Dropout, Add &amp; Norm"></a>Dropout, Add &amp; Norm</h4><script type="math/tex; mode=display">
\text {Layer Norm }(x+\text { Dropout }(\text {Sublayer}(x)))</script><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>这里相较与 <strong>Encoder</strong> 多了：</p>
<p><strong>target input</strong>:目标输入</p>
<h4 id="数据流-1"><a href="#数据流-1" class="headerlink" title="数据流"></a>数据流</h4><ul>
<li><p><strong>Encoder</strong> 数据出来， <strong>input_size * emb_dim</strong></p>
</li>
<li><p>masked之后 <strong>target_length * emb_dim</strong></p>
</li>
<li><p>再输入把 Encoder 的数据传给和masked数据一起传入N个 Decoder ，<strong>target_length * emb_dim</strong></p>
</li>
<li><p>最后通过fully connected layer 和 row-wise softmax 输出 <strong>target * vocab_size</strong></p>
</li>
</ul>
<p>&emsp; <strong><em>note：</em></strong>每一个Block不会共享权重</p>
<p>输入与 <strong>Encoder</strong> 完全相同，与encoder主要的不同在于，target sequence 将左移一位，前面加上标志</p>
<p><ss>，例如：</ss></p>
<blockquote>
<p>[“Hola”, “, “, “como”, “estás”, “?”]→[“<ss>”, “Hola”, “, “, “como”, “estás”, “?”]</ss></p>
</blockquote>
<h4 id="Decode"><a href="#Decode" class="headerlink" title="Decode"></a>Decode</h4><ul>
<li>Test Time</li>
</ul>
<ol>
<li><p>计算embedding representation</p>
</li>
<li><p>使用开始token，比如<ss>作为序列的第一个target。然后模型的输出作为下一个token</ss></p>
</li>
<li><p>把最后一个预测的标记添加到序列之中，并使用他生成下一个新的预测</p>
</li>
<li><p>一直重复步骤3. 知道遇到表示结束的token 比如<eos></eos></p>
</li>
</ol>
<ul>
<li>Training Time</li>
</ul>
<p>按照我们之前的示例，我们将输入：</p>
<blockquote>
<p>[‘<ss>‘，’Hola’，’，’，’como’，’estas’，’？’]</ss></p>
</blockquote>
<p>  预期的预测是：</p>
<blockquote>
<p>[‘Hola’，’，’，’como’，’estas’，’？’，’<eos>‘]</eos></p>
</blockquote>
<p>由于在训练的时候，全部的信息都是已经知道的。我们应该防止通过已经看到的单词来预测单词。例如，它可能会在’como’的右侧看到’estas ‘并用它来预测’estas’。</p>
<p>让我们举一个例子来说明这一点。鉴于：<br>[‘<ss>‘，’Hola’，’，’，’como’，’estas’，’？’]<br>我们将如上所述将其转换为矩阵并添加位置编码。这会产生一个矩阵：</ss></p>
<p><img src="11.png" alt="png"></p>
<p><img src="12.png" alt="png"></p>
<p>如果要预测estas，我们能够信息能够交互的区域就如上图所示。</p>
<h4 id="Masked-Multi-Head-Attention"><a href="#Masked-Multi-Head-Attention" class="headerlink" title="Masked Multi-Head Attention"></a>Masked Multi-Head Attention</h4><p>与<strong>Multi-Head Attention</strong>机制完全相同，但为我们的输入添加maske。而且需要maske的唯一的block是解码器中的第一块，修改将在计算之后。</p>
<script type="math/tex; mode=display">
\frac{Q_{i} K_{i}^{T}}{\sqrt{d_{k}}}</script><p>得到：</p>
<p><img src="13.png" alt="png"></p>
<p>maske步骤只是将矩阵的严格上三角形部分中的所有条目设置为负无穷大。</p>
<p><img src="14.png" alt="png"></p>
<p>其余部分与编码器<strong>Multi-Head Attention</strong>描述的相同。</p>
<p>设置为负无穷之后，通过softmax函数。其后面需要忽略的token的注意力将变为0，忽略了后面的单词。当将此矩阵与<em>V_i</em>相乘时，将用于预测下一个单词的唯一元素是其右侧的元素，即模型在测试时间内可以访问的元素。<img src="15.png" alt="png"></p>
<p>输出将是维度矩阵 <strong>target_length *emb_dim </strong>因为计算它的序列具有 <strong>target_length</strong> 的序列长度</p>
<h4 id="Multi-Head-Attention-—-Encoder-output-and-target"><a href="#Multi-Head-Attention-—-Encoder-output-and-target" class="headerlink" title="Multi-Head Attention — Encoder output and target"></a>Multi-Head Attention — Encoder output and target</h4><p><img src="16.png" alt="png"></p>
<p>与之前 <strong>Multi-Head Attention</strong> 层中那样从<em>X</em>中导出<em>Q_i</em>，<em>K_i</em>和<em>V_i</em>不同，而是使用编码器的最终输出<em>E</em>（所有编码器块的最终结果）和解码器的前一层输出<em>D</em>（经过<strong>Dropout，</strong> <strong>Add＆Norm </strong> <strong>图层后屏蔽</strong> <strong>的Multi-Head Attention</strong>）。</p>
<p>让我们首先澄清这些输入的形状及其代表的含义：</p>
<ol>
<li><em>E</em>，编码输入序列，是维度<em>（input_length）x（emb_dim）</em>的矩阵，其通过经过6个编码器块编码输入令牌之间的关系。</li>
<li><em>D</em>，经过<strong>Add＆Norm</strong>后屏蔽的<strong>Multi-Head Attention</strong>的输出是维度矩阵 <strong>target_length* emb_dim</strong></li>
</ol>
<p>现在让我们深入了解如何处理这些矩阵。我们将使用与以前相同尺寸的加权矩阵：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{W_{i}^{K} \text { with dimensons } d_{e m b_{-} d i m} \mathrm{x} d_{k}} \\ {W_{i}^{Q} \text { with dimensons } d_{e m b_{-} d i m} \mathrm{x} d_{k}} \\ {W_{i}^{V} \text { with dimensons } d_{e m b-d i m} \mathrm{x} d_{v}}\end{array}</script><p>但是这次投影生成 <strong>Q_i</strong> 将使用<strong>D</strong>（目标信息）完成，而生成K和V的投影将使用<strong>E</strong>（输入信息）创建。</p>
<script type="math/tex; mode=display">
\begin{aligned} D W_{i}^{Q} &=Q_{i} \text { with dimensons target length } \mathrm{x} d_{k} \\ E W_{i}^{K} &=K_{i} \text { with dimensons input } \text {length} \mathrm{x} d_{k} \\ E W_{i}^{V} &=V_{i} \text { with dimensons input-length } \mathrm{x} d_{v} \end{aligned}</script><p>同样由多个head，串联之后使用的矩阵W_0将具有与编码器块中使用的尺寸<strong>（d_v * h）x（emb_dim）</strong>一样的尺寸。其他的与编码器中的 <strong>Multi-Head Attention</strong> 完全相同。</p>
<h4 id="Linear-and-Softmax"><a href="#Linear-and-Softmax" class="headerlink" title="Linear and Softmax"></a>Linear and Softmax</h4><p>来自最后一个解码器块的最后一个 <strong>Add＆Norm</strong> 层的输出是维度 <strong>（target_length）x（emb_dim）</strong>的矩阵X</p>
<script type="math/tex; mode=display">
X W_{1}</script><p>并在每个结果行中应用Softmax</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>1.<a href="[https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f     ](https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f">Dissecting BERT Appendix: The Decoder</a>)</p>
<p>2.<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention Is All You Need</a>; Vaswani et al., 2017.</p>
<p>3.<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html#embeddings-and-softmax" target="_blank" rel="noopener">The Annotated Transformer</a>; Alexander Rush, Vincent Nguyen and Guillaume Klein.</p>
<p>4.<a href="https://medium.com/@mromerocalvo/dissecting-bert-part1-6dcf5360b07f" target="_blank" rel="noopener">Dissecting BERT Part 1: Understanding the Transformer</a></p>

    </div>

    
    
    
        
      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/08/25/Skip-Thought Vectors/" rel="next" title="Skip-Thought Vectors">
                  <i class="fa fa-chevron-left"></i> Skip-Thought Vectors
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/09/06/Neural Networks and Neural Language Models/" rel="prev" title="Neural Networks and Neural Language Models">
                  Neural Networks and Neural Language Models <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/echarts.png"
      alt="Les">
  <p class="site-author-name" itemprop="name">Les</p>
  <div class="site-description motion-element" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">36</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">categories</span>
        
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
        
        
          
        
          
        
          
        
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span>
        
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>



        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer"><span class="nav-number">1.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-Architecture"><span class="nav-number">1.1.</span> <span class="nav-text">Model Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder"><span class="nav-number">1.2.</span> <span class="nav-text">Encoder</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#先定义符号："><span class="nav-number">1.2.1.</span> <span class="nav-text">先定义符号：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#数据流"><span class="nav-number">1.2.2.</span> <span class="nav-text">数据流</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Input-Embedding"><span class="nav-number">1.2.3.</span> <span class="nav-text">Input_Embedding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-Head-Attention"><span class="nav-number">1.2.4.</span> <span class="nav-text">Multi-Head Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Feed-Forward-Network"><span class="nav-number">1.2.5.</span> <span class="nav-text">Feed-Forward Network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dropout-Add-amp-Norm"><span class="nav-number">1.2.6.</span> <span class="nav-text">Dropout, Add &amp; Norm</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decoder"><span class="nav-number">1.3.</span> <span class="nav-text">Decoder</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据流-1"><span class="nav-number">1.3.1.</span> <span class="nav-text">数据流</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Decode"><span class="nav-number">1.3.2.</span> <span class="nav-text">Decode</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Masked-Multi-Head-Attention"><span class="nav-number">1.3.3.</span> <span class="nav-text">Masked Multi-Head Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-Head-Attention-—-Encoder-output-and-target"><span class="nav-number">1.3.4.</span> <span class="nav-text">Multi-Head Attention — Encoder output and target</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-and-Softmax"><span class="nav-number">1.3.5.</span> <span class="nav-text">Linear and Softmax</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考"><span class="nav-number">1.4.</span> <span class="nav-text">参考</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Les</span>
</div>

        








        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
      </div>

    

  </div>

  
    
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

<script src="/js/utils.js?v=7.3.0"></script>
  <script src="/js/motion.js?v=7.3.0"></script>


  <script src="/js/affix.js?v=7.3.0"></script>
  <script src="/js/schemes/pisces.js?v=7.3.0"></script>


<script src="/js/next-boot.js?v=7.3.0"></script>




  




























  

  
    
      <script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', function() {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


  
  <script src="/js/scrollspy.js?v=7.3.0"></script>
<script src="/js/post-details.js?v=7.3.0"></script>


</body>
</html>
