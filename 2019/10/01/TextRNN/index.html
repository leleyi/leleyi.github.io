<!DOCTYPE html>





<html class="theme-next pisces use-motion" lang="en">
<head>
  <meta charset="UTF-8">
<meta name="generator" content="Hexo 3.9.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/echarts.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/echarts.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"scrollpercent":true,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    save_scroll: false,
    copycode: {"enable":true,"show_result":true,"style":null},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="TextRNN使用双向LSTM。使用最后的hidden layer全连接进行分类。 12345678910import numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom sklearn import metricsimport timeimport pickle as pklimport">
<meta property="og:type" content="article">
<meta property="og:title" content="TextRNN">
<meta property="og:url" content="http://yoursite.com/2019/10/01/TextRNN/index.html">
<meta property="og:site_name" content="FILE">
<meta property="og:description" content="TextRNN使用双向LSTM。使用最后的hidden layer全连接进行分类。 12345678910import numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom sklearn import metricsimport timeimport pickle as pklimport">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/2019/10/01/TextRNN/textrnn.png">
<meta property="og:updated_time" content="2019-10-04T12:31:21.874Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TextRNN">
<meta name="twitter:description" content="TextRNN使用双向LSTM。使用最后的hidden layer全连接进行分类。 12345678910import numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom sklearn import metricsimport timeimport pickle as pklimport">
<meta name="twitter:image" content="http://yoursite.com/2019/10/01/TextRNN/textrnn.png">
  <link rel="alternate" href="/atom.xml" title="FILE" type="application/atom+xml">
  <link rel="canonical" href="http://yoursite.com/2019/10/01/TextRNN/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>TextRNN | FILE</title>
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  <div class="container sidebar-position-left">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">FILE</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content page-post-detail">
            

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/01/TextRNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Les">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/echarts.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FILE">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">TextRNN

            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-10-01 20:52:15" itemprop="dateCreated datePublished" datetime="2019-10-01T20:52:15+08:00">2019-10-01</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-10-04 20:31:21" itemprop="dateModified" datetime="2019-10-04T20:31:21+08:00">2019-10-04</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="TextRNN"><a href="#TextRNN" class="headerlink" title="TextRNN"></a>TextRNN</h3><p>使用双向LSTM。使用最后的hidden layer全连接进行分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pickle <span class="keyword">as</span> pkl</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> timedelta</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_file_path = <span class="string">"THUCNews/data/train.txt"</span></span><br><span class="line">test_file_path = <span class="string">"THUCNews/data/test.txt"</span></span><br><span class="line">dev_file_path = <span class="string">"THUCNews/data/dev.txt"</span></span><br><span class="line">class_file_path = <span class="string">"THUCNews/data/class.txt"</span></span><br><span class="line">embedding_file_path = <span class="string">"THUCNews/data/embedding_SougouNews.npz"</span></span><br><span class="line">vocab_file_path = <span class="string">"THUCNews/data/vocab.pkl"</span></span><br><span class="line">save_file_path = <span class="string">"THUCNews/saved_dict/TextRNN.ckpt"</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载与训练好的embedding 数组</span></span><br><span class="line">embedding_pretrained = torch.tensor(</span><br><span class="line">            np.load(embedding_file_path)[<span class="string">"embeddings"</span>].astype(<span class="string">'float32'</span>))\</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成一个 字 与 id 的map 相互一一映射(tokenizer 切割方式)</span></span><br><span class="line">UNK, PAD = <span class="string">'&lt;UNK&gt;'</span>, <span class="string">'&lt;PAD&gt;'</span> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_vocab</span><span class="params">(file_path, max_size = <span class="number">10000</span>, min_freq = <span class="number">1</span>)</span>:</span> </span><br><span class="line">    vocab_dic = &#123;&#125;</span><br><span class="line">    tokenizer = <span class="keyword">lambda</span> x:[y <span class="keyword">for</span>  y <span class="keyword">in</span> x] <span class="comment"># 切割</span></span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>, encoding = <span class="string">'UTF-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(f):</span><br><span class="line">            content = line.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="comment"># 将 文本与文本类型标志 分开</span></span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> tokenizer(content):</span><br><span class="line">                vocab_dic[word] = vocab_dic.get(word, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        vocab_list = sorted([it <span class="keyword">for</span> it <span class="keyword">in</span> vocab_dic.items() <span class="keyword">if</span> it[<span class="number">1</span>] &gt; min_freq], key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>], reverse = <span class="literal">True</span>)[:max_size] </span><br><span class="line">        vocab_dic = &#123;word_count[<span class="number">0</span>]: idx <span class="keyword">for</span> idx, word_count <span class="keyword">in</span> enumerate(vocab_list)&#125;</span><br><span class="line">        vocab_dic.update(&#123;UNK: len(vocab_dic), PAD: len(vocab_dic) + <span class="number">1</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> vocab_dic</span><br></pre></td></tr></table></figure>
<blockquote>
<p>{“如”：           213}  -&gt;  213    embedding   [1.24, 2.1, 3.5, 2.52,…… 12.3, 0.234]<br>{“<unk>“：4762}   -&gt; 4762   embedding   [3.04, 3.3, 1.5, 0.52,…… 2.13, 0.341] </unk></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成与源文本与之对应的 id 数值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">(file_path, pad_size=<span class="number">32</span>)</span>:</span></span><br><span class="line">    contents = []</span><br><span class="line">    tokenizer = <span class="keyword">lambda</span> x:[y <span class="keyword">for</span>  y <span class="keyword">in</span> x] <span class="comment"># 切割</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(vocab_file_path):</span><br><span class="line">        vocab = pkl.load(open(vocab_file_path, <span class="string">'rb'</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        vocab = get_vocab(train_file_path, <span class="number">10000</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>, encoding = <span class="string">'UTF-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(f):</span><br><span class="line">            content, label = line.split(<span class="string">'\t'</span>)</span><br><span class="line">            words_line = []</span><br><span class="line">            token = tokenizer(content)</span><br><span class="line">            seq_len = len(token)</span><br><span class="line">            <span class="keyword">if</span> pad_size:</span><br><span class="line">                <span class="keyword">if</span> len(token) &lt; pad_size:</span><br><span class="line">                    token.extend([vocab.get(PAD)] * (pad_size - len(token)))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    token = token[:pad_size]</span><br><span class="line">                    seq_len = pad_size</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> token:</span><br><span class="line">                words_line.append(vocab.get(word, vocab.get(UNK)))</span><br><span class="line">            contents.append((words_line, int(label), seq_len))</span><br><span class="line">    <span class="keyword">return</span> contents</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 产生train dev 和 test 数据集合（由id组成，并与文字一一对应）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    train = load_dataset(train_file_path, <span class="number">32</span>)</span><br><span class="line">    dev = load_dataset(dev_file_path, <span class="number">32</span>)</span><br><span class="line">    test = load_dataset(test_file_path, <span class="number">32</span>)</span><br><span class="line">    <span class="keyword">return</span> train, dev, test</span><br></pre></td></tr></table></figure>
<blockquote>
<p>（[“我”, “爱”, “中”,  “国”，”,”…….. “美”， “好”，“<pad>”，“<pad>”]，  类型，总的长度）<br>（[132,   3,      32,    44,    24,……, 32,      213,      4403,          4403 ] ，       3，   30）</pad></pad></p>
</blockquote>
<p><img src="textrnn.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(embedding_pretrained, freeze=<span class="literal">False</span>)</span><br><span class="line">        self.lstm = nn.LSTM(<span class="number">300</span>, <span class="number">32</span>, bidirectional=<span class="literal">True</span>, batch_first=<span class="literal">True</span>, dropout=<span class="number">0.5</span>) <span class="comment">#hidden_size 32</span></span><br><span class="line">        self.fc = nn.Linear(<span class="number">32</span>* <span class="number">2</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># x[0] shape 128 * 32</span></span><br><span class="line">        out = self.embedding(x[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># embedding 之后 32个 每一个都转换为 300 维度的向量</span></span><br><span class="line">        out, h_n = self.lstm(out) </span><br><span class="line">        <span class="comment"># 得到一个shape 128 * 32 * 256 -&gt; 32个字 由于是双向 每一个字对应两个hidden 所以拼起来就是32 * 256</span></span><br><span class="line">        out = self.fc(out[:, <span class="number">-1</span>, :]) <span class="comment"># 取最后一共hidden层</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_iterator</span><span class="params">(dataset, batch_size)</span>:</span></span><br><span class="line">    iter = DatasetIterater(dataset, batch_size, <span class="string">'cpu'</span>)</span><br><span class="line">    <span class="keyword">return</span> iter</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 权重初始化， 使用xavier</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_network</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> name, w <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">         <span class="keyword">if</span> <span class="string">'embedding'</span> <span class="keyword">not</span> <span class="keyword">in</span> name: </span><br><span class="line">            <span class="keyword">if</span> <span class="string">'weight'</span> <span class="keyword">in</span> name:</span><br><span class="line">                    nn.init.xavier_normal_(w)</span><br><span class="line">            <span class="keyword">elif</span> <span class="string">'bias'</span> <span class="keyword">in</span> name:</span><br><span class="line">                nn.init.constant_(w, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, train_iter, dev_iter, test_iter)</span>:</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    model.train() <span class="comment"># 训练模式</span></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr =  <span class="number">1e-3</span>)</span><br><span class="line">    batch_no = <span class="number">0</span> <span class="comment"># 记录到多少batch了</span></span><br><span class="line">    dev_best_loss = float(<span class="string">'inf'</span>)</span><br><span class="line">    last_improve = <span class="number">0</span> <span class="comment"># 上次验证集loss下降的batch数</span></span><br><span class="line">    flag = <span class="literal">False</span> <span class="comment"># 是否很久没有效果提升</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        print(<span class="string">'Epoch [&#123;&#125;/&#123;&#125;]'</span>.format(epoch + <span class="number">1</span>, <span class="number">20</span>))</span><br><span class="line">        <span class="keyword">for</span> i, (trains, labels) <span class="keyword">in</span> enumerate(train_iter):</span><br><span class="line">            outputs = model(trains)</span><br><span class="line">            model.zero_grad()</span><br><span class="line">            loss = F.cross_entropy(outputs, labels) <span class="comment"># 交叉熵损失函数</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> batch_no % <span class="number">100</span> == <span class="number">0</span>: <span class="comment"># 100输出 训练集，和验证集的效果</span></span><br><span class="line">                true = labels.data.cpu()</span><br><span class="line">                predic = torch.max(outputs.data, <span class="number">1</span>)[<span class="number">1</span>].cpu() <span class="comment"># 获取输出结果中最大的作为预测类别</span></span><br><span class="line">                train_acc = metrics.accuracy_score(true, predic) <span class="comment"># 获取训练集的准确度</span></span><br><span class="line">                dev_acc, dev_loss = evaluate(model, dev_iter) <span class="comment"># 评估验证集中的效果</span></span><br><span class="line">                <span class="keyword">if</span> dev_loss &lt; dev_best_loss:</span><br><span class="line">                    dev_best_loss = dev_loss</span><br><span class="line">                    torch.save(model.state_dict(), save_file_path) <span class="comment"># 将有提升的模型参数存下来</span></span><br><span class="line">                    improve = <span class="string">'*'</span></span><br><span class="line">                    last_improve = batch_no <span class="comment"># 最后提升的批次号</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    improve = <span class="string">''</span></span><br><span class="line">                time_dif = time.time() - start_time <span class="comment"># 记录花费的时间</span></span><br><span class="line">                msg = <span class="string">'Iter: &#123;0:&gt;6&#125;,  Train Loss: &#123;1:&gt;5.2&#125;,  Train Acc: &#123;2:&gt;6.2%&#125;,  '</span> \</span><br><span class="line">                      <span class="string">'Val Loss: &#123;3:&gt;5.2&#125;,  Val Acc: &#123;4:&gt;6.2%&#125;,  Time: &#123;5&#125; &#123;6&#125;'</span></span><br><span class="line">                print(msg.format(batch_no, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve)) <span class="comment"># </span></span><br><span class="line">                model.train()</span><br><span class="line">            batch_no += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> batch_no - last_improve &gt; <span class="number">1000</span>: <span class="comment"># 如果已经超过1000次没有提升了 就主动停止训练</span></span><br><span class="line">                print(<span class="string">"No improve, auto-stoppping"</span>)</span><br><span class="line">                flag = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> flag:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    test(model, test_iter)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(model, test_iter)</span>:</span></span><br><span class="line">    model.load_state_dict(torch.load(save_file_path)) <span class="comment"># 加载以及存储的dev 效果最好的相应模型</span></span><br><span class="line">    model.eval()</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    test_acc, test_loss, test_report, test_confusion = evaluate(model, test_iter, test=<span class="literal">True</span>)</span><br><span class="line">    msg = <span class="string">'Test Loss: &#123;0:&gt;5.2&#125;,  Test Acc: &#123;1:&gt;6.2%&#125;'</span></span><br><span class="line">    print(msg.format(test_loss, test_acc))</span><br><span class="line">    print(<span class="string">"Precision, Recall and F1-Score..."</span>)</span><br><span class="line">    print(test_report)</span><br><span class="line">    print(<span class="string">"Confusion Matrix..."</span>)</span><br><span class="line">    print(test_confusion)</span><br><span class="line">    time_dif = time.time() - start_time</span><br><span class="line">    print(<span class="string">"Time usage:"</span>, time_dif)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, data_iter, test = False)</span>:</span></span><br><span class="line">    model.eval() <span class="comment"># eval 模式</span></span><br><span class="line">    loss_total = <span class="number">0</span></span><br><span class="line">    predict_all = np.array([], dtype = int)</span><br><span class="line">    labels_all = np.array([], dtype = int)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> texts, labels <span class="keyword">in</span> data_iter:</span><br><span class="line">            outputs = model(texts)</span><br><span class="line">            loss = F.cross_entropy(outputs, labels)</span><br><span class="line">            loss_total += loss</span><br><span class="line">            labels = labels.data.cpu().numpy()</span><br><span class="line">            predic = torch.max(outputs.data, <span class="number">1</span>)[<span class="number">1</span>].cpu().numpy()</span><br><span class="line">            labels_all = np.append(labels_all, labels) <span class="comment"># 验证集上一个批次真实分类</span></span><br><span class="line">            predict_all = np.append(predict_all, predic) <span class="comment"># 验证集上一个批次的预测结果</span></span><br><span class="line">    acc = metrics.accuracy_score(labels_all, predict_all) <span class="comment"># 在整个验证集上的acc</span></span><br><span class="line">    <span class="keyword">if</span> test:</span><br><span class="line">        class_list = [x.strip() <span class="keyword">for</span> x <span class="keyword">in</span> open(</span><br><span class="line">             <span class="string">'THUCNews/data/class.txt'</span>).readlines()] </span><br><span class="line">        report = metrics.classification_report(labels_all, predict_all, target_names = class_list, digits=<span class="number">4</span>)</span><br><span class="line">        confusion = metrics.confusion_matrix(labels_all, predict_all)</span><br><span class="line">        <span class="keyword">return</span> acc, loss_total / len(data_iter), report, confusion</span><br><span class="line">    <span class="keyword">return</span> acc, loss_total / len(data_iter)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">()</span>:</span></span><br><span class="line">    torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">    torch.cuda.manual_seed_all(<span class="number">1</span>)</span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span>  <span class="comment"># 保证每次结果一样</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    print(<span class="string">"Loading data..."</span>) <span class="comment">#vocab, </span></span><br><span class="line">    train_data, dev_data, test_data = build_dataset()</span><br><span class="line">    print(<span class="string">"+++++++++++++++++++++++++++++++++"</span>)</span><br><span class="line">    train_iter = build_iterator(train_data, <span class="number">128</span>) <span class="comment">#batch_size</span></span><br><span class="line">    print(train_iter)</span><br><span class="line">    dev_iter = build_iterator(dev_data, <span class="number">128</span>)</span><br><span class="line">    print(train_iter)</span><br><span class="line">    test_iter = build_iterator(test_data, <span class="number">128</span>)</span><br><span class="line">    print(train_iter)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"Loading data Complete, Using time"</span>, time.time() - start_time)</span><br><span class="line">    </span><br><span class="line">    model = Model()</span><br><span class="line">    init_network(model)</span><br><span class="line">    print(model.parameters)</span><br><span class="line">    </span><br><span class="line">    train(model, train_iter, dev_iter, test_iter)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run()</span><br></pre></td></tr></table></figure>
<pre><code>Loading data...


180000it [00:02, 65875.25it/s]
10000it [00:00, 70955.32it/s]
10000it [00:00, 69962.09it/s]


+++++++++++++++++++++++++++++++++
&lt;__main__.DatasetIterater object at 0x000002854FC59D30&gt;
&lt;__main__.DatasetIterater object at 0x000002854FC59D30&gt;
&lt;__main__.DatasetIterater object at 0x000002854FC59D30&gt;
Loading data Complete, Using time 3.046241283416748


C:\ProgramData\Anaconda3\lib\site-packages\torch\nn\modules\rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  &quot;num_layers={}&quot;.format(dropout, num_layers))


&lt;bound method Module.parameters of Model(
  (embedding): Embedding(4762, 300)
  (lstm): LSTM(300, 32, batch_first=True, dropout=0.5, bidirectional=True)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)&gt;
Epoch [1/20]
Iter:      0,  Train Loss:   2.3,  Train Acc:  5.47%,  Val Loss:   2.3,  Val Acc:  9.99%,  Time: 3.8678135871887207 *
Iter:    100,  Train Loss:   1.9,  Train Acc: 28.91%,  Val Loss:   1.9,  Val Acc: 31.62%,  Time: 22.469587802886963 *
Iter:    200,  Train Loss:   1.4,  Train Acc: 51.56%,  Val Loss:   1.2,  Val Acc: 55.65%,  Time: 40.84703755378723 *
Iter:    300,  Train Loss:  0.95,  Train Acc: 72.66%,  Val Loss:   1.1,  Val Acc: 65.07%,  Time: 60.29182720184326 *
Iter:    400,  Train Loss:  0.78,  Train Acc: 75.78%,  Val Loss:  0.81,  Val Acc: 74.96%,  Time: 78.93504977226257 *
Iter:    500,  Train Loss:  0.63,  Train Acc: 82.81%,  Val Loss:  0.72,  Val Acc: 77.39%,  Time: 98.05553436279297 *
Iter:    600,  Train Loss:  0.69,  Train Acc: 79.69%,  Val Loss:  0.67,  Val Acc: 79.05%,  Time: 117.16351222991943 *
Iter:    700,  Train Loss:  0.63,  Train Acc: 82.03%,  Val Loss:  0.61,  Val Acc: 81.13%,  Time: 135.9926619529724 *
Iter:    800,  Train Loss:   0.5,  Train Acc: 83.59%,  Val Loss:  0.57,  Val Acc: 82.46%,  Time: 155.52704286575317 *
Iter:    900,  Train Loss:  0.52,  Train Acc: 83.59%,  Val Loss:  0.55,  Val Acc: 82.95%,  Time: 174.2003309726715 *
Iter:   1000,  Train Loss:  0.44,  Train Acc: 86.72%,  Val Loss:  0.54,  Val Acc: 83.21%,  Time: 192.82258319854736 *
Iter:   1100,  Train Loss:   0.4,  Train Acc: 89.84%,  Val Loss:  0.49,  Val Acc: 84.65%,  Time: 211.5442943572998 *
Iter:   1200,  Train Loss:  0.36,  Train Acc: 89.06%,  Val Loss:  0.48,  Val Acc: 85.20%,  Time: 230.28150415420532 *
Iter:   1300,  Train Loss:  0.48,  Train Acc: 83.59%,  Val Loss:  0.48,  Val Acc: 85.06%,  Time: 249.9981701374054 *
Iter:   1400,  Train Loss:  0.55,  Train Acc: 84.38%,  Val Loss:  0.46,  Val Acc: 85.76%,  Time: 268.71038818359375 *
Epoch [2/20]
Iter:   1500,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.45,  Val Acc: 85.83%,  Time: 287.3316514492035 *
Iter:   1600,  Train Loss:  0.45,  Train Acc: 85.16%,  Val Loss:  0.46,  Val Acc: 85.38%,  Time: 306.0338559150696 
Iter:   1700,  Train Loss:  0.42,  Train Acc: 84.38%,  Val Loss:  0.43,  Val Acc: 86.62%,  Time: 324.7470557689667 *
Iter:   1800,  Train Loss:  0.34,  Train Acc: 90.62%,  Val Loss:  0.44,  Val Acc: 86.25%,  Time: 343.53873229026794 
Iter:   1900,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.41,  Val Acc: 86.96%,  Time: 362.429979801178 *
Iter:   2000,  Train Loss:  0.41,  Train Acc: 87.50%,  Val Loss:  0.41,  Val Acc: 86.97%,  Time: 381.4939856529236 *
Iter:   2100,  Train Loss:  0.45,  Train Acc: 87.50%,  Val Loss:   0.4,  Val Acc: 87.39%,  Time: 400.5055284500122 *
Iter:   2200,  Train Loss:  0.28,  Train Acc: 91.41%,  Val Loss:  0.41,  Val Acc: 87.08%,  Time: 419.2572121620178 
Iter:   2300,  Train Loss:  0.32,  Train Acc: 90.62%,  Val Loss:   0.4,  Val Acc: 87.43%,  Time: 438.2492642402649 *
Iter:   2400,  Train Loss:  0.31,  Train Acc: 91.41%,  Val Loss:   0.4,  Val Acc: 87.65%,  Time: 456.98245310783386 *
Iter:   2500,  Train Loss:  0.28,  Train Acc: 90.62%,  Val Loss:   0.4,  Val Acc: 86.99%,  Time: 476.24933409690857 
Iter:   2600,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.39,  Val Acc: 87.81%,  Time: 495.0570206642151 *
Iter:   2700,  Train Loss:  0.27,  Train Acc: 93.75%,  Val Loss:  0.39,  Val Acc: 87.61%,  Time: 513.9666152000427 *
Iter:   2800,  Train Loss:  0.44,  Train Acc: 85.94%,  Val Loss:  0.38,  Val Acc: 87.87%,  Time: 532.8607122898102 *
Epoch [3/20]
Iter:   2900,  Train Loss:   0.4,  Train Acc: 89.06%,  Val Loss:  0.38,  Val Acc: 87.80%,  Time: 551.6988520622253 
Iter:   3000,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:  0.38,  Val Acc: 88.30%,  Time: 570.5799615383148 *
Iter:   3100,  Train Loss:  0.28,  Train Acc: 89.84%,  Val Loss:  0.39,  Val Acc: 87.66%,  Time: 589.4430770874023 
Iter:   3200,  Train Loss:  0.35,  Train Acc: 90.62%,  Val Loss:  0.38,  Val Acc: 87.63%,  Time: 608.1702740192413 
Iter:   3300,  Train Loss:  0.37,  Train Acc: 85.94%,  Val Loss:  0.38,  Val Acc: 87.97%,  Time: 627.0054371356964 
Iter:   3400,  Train Loss:   0.3,  Train Acc: 89.84%,  Val Loss:  0.38,  Val Acc: 88.29%,  Time: 645.7606160640717 
Iter:   3500,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.37,  Val Acc: 88.25%,  Time: 665.3868064880371 *
Iter:   3600,  Train Loss:   0.2,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 88.15%,  Time: 684.3798725605011 
Iter:   3700,  Train Loss:  0.37,  Train Acc: 84.38%,  Val Loss:  0.36,  Val Acc: 88.40%,  Time: 703.2969629764557 *
Iter:   3800,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.36,  Val Acc: 88.65%,  Time: 722.1076171398163 
Iter:   3900,  Train Loss:  0.33,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 88.56%,  Time: 741.0077271461487 
Iter:   4000,  Train Loss:  0.24,  Train Acc: 91.41%,  Val Loss:  0.37,  Val Acc: 88.10%,  Time: 759.8083894252777 
Iter:   4100,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.37,  Val Acc: 88.56%,  Time: 778.7774691581726 
Iter:   4200,  Train Loss:  0.34,  Train Acc: 92.19%,  Val Loss:  0.35,  Val Acc: 88.69%,  Time: 797.5661556720734 *
Epoch [4/20]
Iter:   4300,  Train Loss:  0.26,  Train Acc: 92.19%,  Val Loss:  0.36,  Val Acc: 88.57%,  Time: 816.4322924613953 
Iter:   4400,  Train Loss:  0.15,  Train Acc: 93.75%,  Val Loss:  0.36,  Val Acc: 88.61%,  Time: 835.2734205722809 
Iter:   4500,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.35,  Val Acc: 88.54%,  Time: 854.1645164489746 *
Iter:   4600,  Train Loss:  0.22,  Train Acc: 92.19%,  Val Loss:  0.36,  Val Acc: 88.29%,  Time: 873.123607635498 
Iter:   4700,  Train Loss:  0.37,  Train Acc: 89.06%,  Val Loss:  0.36,  Val Acc: 88.73%,  Time: 892.0637054443359 
Iter:   4800,  Train Loss:  0.18,  Train Acc: 92.19%,  Val Loss:  0.36,  Val Acc: 88.60%,  Time: 911.1337130069733 
Iter:   4900,  Train Loss:  0.22,  Train Acc: 92.97%,  Val Loss:  0.36,  Val Acc: 88.87%,  Time: 930.1882197856903 
Iter:   5000,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.36,  Val Acc: 88.76%,  Time: 949.0653252601624 
Iter:   5100,  Train Loss:  0.28,  Train Acc: 88.28%,  Val Loss:  0.35,  Val Acc: 88.85%,  Time: 968.1674032211304 *
Iter:   5200,  Train Loss:  0.33,  Train Acc: 86.72%,  Val Loss:  0.35,  Val Acc: 88.98%,  Time: 987.1639504432678 
Iter:   5300,  Train Loss:   0.2,  Train Acc: 91.41%,  Val Loss:  0.35,  Val Acc: 88.65%,  Time: 1005.9341161251068 
Iter:   5400,  Train Loss:  0.34,  Train Acc: 89.84%,  Val Loss:  0.35,  Val Acc: 89.01%,  Time: 1024.9731514453888 
Iter:   5500,  Train Loss:  0.22,  Train Acc: 92.19%,  Val Loss:  0.35,  Val Acc: 88.74%,  Time: 1045.3965137004852 
Iter:   5600,  Train Loss:  0.16,  Train Acc: 96.88%,  Val Loss:  0.35,  Val Acc: 88.91%,  Time: 1065.713327884674 
Epoch [5/20]
Iter:   5700,  Train Loss:  0.29,  Train Acc: 90.62%,  Val Loss:  0.35,  Val Acc: 88.90%,  Time: 1085.443971157074 
Iter:   5800,  Train Loss:  0.17,  Train Acc: 95.31%,  Val Loss:  0.35,  Val Acc: 88.97%,  Time: 1104.4659898281097 
Iter:   5900,  Train Loss:  0.24,  Train Acc: 90.62%,  Val Loss:  0.36,  Val Acc: 88.75%,  Time: 1124.85223031044 
Iter:   6000,  Train Loss:  0.28,  Train Acc: 90.62%,  Val Loss:  0.36,  Val Acc: 88.90%,  Time: 1146.5233688354492 
Iter:   6100,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.36,  Val Acc: 88.79%,  Time: 1166.4595756530762 
No improve, auto-stoppping
Test Loss:  0.35,  Test Acc: 89.18%
Precision, Recall and F1-Score...
               precision    recall  f1-score   support

      finance     0.9049    0.8470    0.8750      1000
       realty     0.8627    0.9360    0.8978      1000
       stocks     0.8492    0.7770    0.8115      1000
    education     0.9258    0.9360    0.9309      1000
      science     0.8071    0.8620    0.8337      1000
      society     0.9007    0.8890    0.8948      1000
     politics     0.8752    0.8770    0.8761      1000
       sports     0.9706    0.9560    0.9632      1000
         game     0.9197    0.9160    0.9178      1000
entertainment     0.9084    0.9220    0.9151      1000

    micro avg     0.8918    0.8918    0.8918     10000
    macro avg     0.8924    0.8918    0.8916     10000
 weighted avg     0.8924    0.8918    0.8916     10000

Confusion Matrix...
[[847  36  67   5  21   8  12   1   2   1]
 [  9 936  12   2   8   9   6   3   4  11]
 [ 58  44 777   1  58   1  44   1  13   3]
 [  0   5   3 936  14  16  13   0   4   9]
 [  4  13  25   8 862  16  16   3  38  15]
 [  5  22   1  24  16 889  24   2   4  13]
 [  6  12  19  18  22  29 877   2   4  11]
 [  1   3   3   2   3   5   5 956   1  21]
 [  2   2   6   2  52   7   2   2 916   9]
 [  4  12   2  13  12   7   3  15  10 922]]
Time usage: 3.51796293258667
</code></pre>
    </div>

    
    
    
        
      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/09/25/TextCNN/" rel="next" title="TextCNN">
                  <i class="fa fa-chevron-left"></i> TextCNN
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/10/09/设计模式/" rel="prev" title="设计模式_单例">
                  设计模式_单例 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/echarts.png"
      alt="Les">
  <p class="site-author-name" itemprop="name">Les</p>
  <div class="site-description motion-element" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">25</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span>
        
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>



        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#TextRNN"><span class="nav-number">1.</span> <span class="nav-text">TextRNN</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Les</span>
</div>

        








        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
      </div>

    

  </div>

  
    
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

<script src="/js/utils.js?v=7.3.0"></script>
  <script src="/js/motion.js?v=7.3.0"></script>


  <script src="/js/affix.js?v=7.3.0"></script>
  <script src="/js/schemes/pisces.js?v=7.3.0"></script>


<script src="/js/next-boot.js?v=7.3.0"></script>




  




























  

  
    
      <script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', function() {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


  
  <script src="/js/scrollspy.js?v=7.3.0"></script>
<script src="/js/post-details.js?v=7.3.0"></script>


</body>
</html>
