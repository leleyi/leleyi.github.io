<!DOCTYPE html>





<html class="theme-next pisces use-motion" lang="en">
<head>
  <meta charset="UTF-8">
<meta name="generator" content="Hexo 3.9.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/echarts.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/echarts.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"scrollpercent":true,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    save_scroll: false,
    copycode: {"enable":true,"show_result":true,"style":null},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="[原文连接]https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d 近年来，单词嵌入（将单词映射到数值向量空间中）已被证明是自然语言处理（NLP）任务的一种非常重要的方法，它使各种依赖于向量表示作为输入的机器学习模型都可以享受更丰富的文本表示输入。这些表示形式保留了更多关于单词的语义和句法信息，从而导致几乎">
<meta property="og:type" content="article">
<meta property="og:title" content="Document_Embedding">
<meta property="og:url" content="http://yoursite.com/2020/08/07/document_embedding/index.html">
<meta property="og:site_name" content="FILE">
<meta property="og:description" content="[原文连接]https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d 近年来，单词嵌入（将单词映射到数值向量空间中）已被证明是自然语言处理（NLP）任务的一种非常重要的方法，它使各种依赖于向量表示作为输入的机器学习模型都可以享受更丰富的文本表示输入。这些表示形式保留了更多关于单词的语义和句法信息，从而导致几乎">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/1.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/2.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/3.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/4.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/5.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/6.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/7.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/8.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/9.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/10.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/11.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/12.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/13.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/14.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/15.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/16.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/17.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/19.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/18.png">
<meta property="og:image" content="http://yoursite.com/2020/08/07/document_embedding/20.png">
<meta property="og:updated_time" content="2021-04-01T13:19:11.070Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Document_Embedding">
<meta name="twitter:description" content="[原文连接]https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d 近年来，单词嵌入（将单词映射到数值向量空间中）已被证明是自然语言处理（NLP）任务的一种非常重要的方法，它使各种依赖于向量表示作为输入的机器学习模型都可以享受更丰富的文本表示输入。这些表示形式保留了更多关于单词的语义和句法信息，从而导致几乎">
<meta name="twitter:image" content="http://yoursite.com/2020/08/07/document_embedding/1.png">
  <link rel="alternate" href="/atom.xml" title="FILE" type="application/atom+xml">
  <link rel="canonical" href="http://yoursite.com/2020/08/07/document_embedding/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Document_Embedding | FILE</title>
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  <div class="container sidebar-position-left">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">FILE</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content page-post-detail">
            

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/07/document_embedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Les">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/echarts.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FILE">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">Document_Embedding

            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-08-07 16:57:15" itemprop="dateCreated datePublished" datetime="2020-08-07T16:57:15+08:00">2020-08-07</time>
            </span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>[原文连接]<a href="https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d" target="_blank" rel="noopener">https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d</a></p>
<p>近年来，单词嵌入（将单词映射到数值向量空间中）已被证明是自然语言处理（NLP）任务的一种非常重要的方法，它使各种依赖于向量表示作为输入的机器学习模型都可以享受更丰富的文本表示输入。这些表示形式保留了更多关于单词的语义和句法信息，从而导致几乎所有可以想象的NLP任务的性能得到改善。</p>
<p>新颖的想法本身及其巨大的影响，都促使研究人员考虑如何将这种丰富的矢量表示形式提供给更大的文本单位（从句子到书籍）的问题。这项工作导致产生了许多新方法来生成这些映射，并为该问题提供了各种创新解决方案以及一些显着的突破。</p>
<blockquote>
<p><strong>注意</strong>”：这里使用“ <strong>文档</strong>”一词来指代<strong>任何单词序列</strong>，从句子和段落到社交媒体帖子，一直到文章，书籍和结构更复杂的文本文档。</p>
</blockquote>
<p>在本文中，我不仅会介绍单词嵌入技术的直接扩展方法（例如<em>doc2vec</em> 扩展<em>word2vec</em> 的方法），还将介绍其他值得注意的技术，这些技术有时会在其他输出中生成ℝⁿ中的文档到向量的映射。 。</p>
<h2 id="文件嵌入的应用"><a href="#文件嵌入的应用" class="headerlink" title="文件嵌入的应用"></a>文件嵌入的应用</h2><p>将文档映射到信息矢量表示的能力具有广泛的应用。以下只是部分列表。</p>
<p>[<a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" target="_blank" rel="noopener">Le &amp; Mikolov, 2014</a>]展示了他们的<em>段落向量</em>法在几种文本分类和情感分析任务上的功能，而[<a href="https://arxiv.org/pdf/1507.07998.pdf" target="_blank" rel="noopener">Dai et al, 2015</a>] 在文档相似性任务的背景下对其进行了检验， [<a href="https://arxiv.org/pdf/1607.05368.pdf" target="_blank" rel="noopener">Lau &amp; Baldwin, 2016</a>]进行了基准测试它针对论坛问题重复任务和 <a href="http://ixa2.si.ehu.es/stswiki/index.php/Main_Page" target="_blank" rel="noopener">the Semantic Textual Similarity (STS) SemEval shared task</a>.</p>
<p>[<a href="https://arxiv.org/abs/1506.06726" target="_blank" rel="noopener">Kiros et al, 2015</a>] 已经证明了他们的<em>Skip-thought</em>向量在语义相关性，释义检测，图像句子排名，问题类型分类以及四个情感和主观性数据集上的使用。[Broere，2017]使用它们来预测POS标签和依赖关系。</p>
<p>[<a href="https://arxiv.org/pdf/1810.09302.pdf" target="_blank" rel="noopener">Chen et al, 2018</a>] 显示了<em>BioSentVec</em> ，他们在生物医学文本上训练过的句子嵌入集，在句子对相似性任务(<a href="https://github.com/ncbi-nlp/BioSentVec" target="_blank" rel="noopener">official Python implementation</a>).上表现良好。</p>
<p>最后，<a href="https://www.microsoft.com/en-us/research/project/dssm/" target="_blank" rel="noopener">Deep Semantic Similarity Model was used by various authors</a> 进行信息检索和Web搜索排名，广告选择/相关性，上下文实体搜索和兴趣度任务，问题解答，知识推断，图像字幕和机器翻译任务。</p>
<p>但是请注意，尽管文档嵌入的问题已经很久了，但许多当前有影响力的解决方案还是很年轻的，并且在紧随当代基于编码器-解码器的单词成功之后，这一领域在最近（大约在2014年）开始兴起嵌入技术，所以现在还很早。话虽如此，我希望这部分内容可以将以下各节放在更广阔的背景下，并以有意义的方式进行组织。</p>
<h2 id="突出的方法和趋势"><a href="#突出的方法和趋势" class="headerlink" title="突出的方法和趋势"></a>突出的方法和趋势</h2><hr>
<h3 id="文本嵌入的方法"><a href="#文本嵌入的方法" class="headerlink" title="文本嵌入的方法"></a>文本嵌入的方法</h3><p>映射领域的一种可能方法是采用以下四种主要方法：</p>
<ol>
<li><strong>总结词向量</strong><br>这是<em>对</em>经典的方法。<em>Bag-of-words</em> 正是针对一个热门单词向量而做到的，而您可以应用到它的各种加权方案都是以这种方式总结单词向量的方式。但是，这种方法在与最先进的词表示形式一起使用时（通常通过求平均值而不是求和）也有效，尤其是在考虑到这种用法优化词嵌入时，并且可以与任何这里介绍的更性感的方法。</li>
<li><strong>主题建模</strong><br>虽然这通常不是主题建模技术（如LDA和PLSI）的主要应用，但它们<em>固有地生成了一个文档嵌入空间</em>，用于对语料库中的单词分布进行建模和解释，而维可以看作是隐藏在<em>文档</em>中的潜在语义结构。数据，因此在我们的上下文中很有用。我没有在本文中真正介绍这种方法（LDA的简要介绍除外），因为我认为LDA很好地代表了这种方法，并且众所周知。</li>
<li><strong>编码器-解码器模型</strong><br>这是场景中最新的无监督功能，具有<em>doc2vec</em> 和 <em>skip-thought</em> 之类的功能。尽管这种方法自2000年代初就出现了（以 <em>神经概率语言模型</em> 的名义），但随着其成功地应用于词嵌入生成，它最近获得了新生，目前的研究集中在如何将其用途扩展到文档嵌入。这种方法从大型未标记语料库的可用性不断提高中获得了比其他方法更多的收益。</li>
<li><strong>有监督的表示学习</strong><br>这种方法的应用源于神经网络模型的兴起（或兴起），以及它们使用各种非线性多层算子学习输入数据的丰富表示的能力，<a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" target="_blank" rel="noopener">该函数可以近似各种映射</a>.。通过简单地将单词袋输入到神经网络中以解决与文本相关的监督问题，您将得到一个模型，其中隐藏层包含输入文本的丰富表示形式，这正是我们所追求的。</li>
</ol>
<p>有几种不受监督的方法不适用于上述任何一种方法（特别是<em>想到的是快速思考</em>和<em>Word Mover的距离</em>），但是我认为大多数技术确实属于这四大类之一。</p>
<blockquote>
<p><strong>注意：</strong>虽然很容易指出经典的单词袋技术遭受独特的顺序信息缺失的困扰，但这实际上是规则，而不是例外。通过本文回顾的大多数新方法获得的主要信息是将分布假设扩展到更大的文本单元。基于神经网络的序列模型是例外。</p>
</blockquote>
<h3 id="挑战和趋势"><a href="#挑战和趋势" class="headerlink" title="挑战和趋势"></a>挑战和趋势</h3><p>整体上研究文档嵌入技术的研究和应用时，会出现几种广泛的趋势，并且可能会发现一些挑战。</p>
<ol>
<li><strong>编码器-解码器优化：研究</strong>的重要部分集中在优化无监督的精确体系结构（例如ANN / CNN / RNN）和某些组件/超参数（例如n-gram，投影函数，权重等）上编码器-解码器方法，用于学习文档嵌入。尽管此微调的目标之一是提高各种任务的成功指标，但目标还是能够在较大的语料库上或更短的时间内训练模型。</li>
<li><strong>学习目标设计：</strong>无监督（或自我监督）表示学习的关键在于设计一个学习目标，该目标利用数据中自由可用的标签，从而生成对下游任务有用的表示。对我来说，这是最令人兴奋的趋势，并且我认为对NLP任务影响最大的趋势可能等同于一个词嵌入技术。目前，我仅将<em>快速思考</em>和<em>Word Mover的距离</em>视为编码器/解码器方法的替代方法。这种趋势的另一个吸引人的方面是，此处的创新也可能适用于词嵌入问题。</li>
<li><strong>基准测试：</strong>一般来说，机器学习研究是整个领域趋势的一部分，文档嵌入（可能由于它是一个年轻的子领域）很好地证明了研究在越来越广泛的领域对技术基准进行研究的重点（请参阅<a href="https://gluebenchmark.com/leaderboard" target="_blank" rel="noopener">GLUE排行榜</a>)。但是，几乎所有关于该主题的论文都宣称与当前SOTA技术具有可比或更好的结果，但这尚未导致明显的领先者出现。</li>
<li><strong>开放源代码：</strong>再次，这是更广泛趋势的一部分，大量使用易于使用的代码实现技术（通常还包括实验）来实现可重现性，并推动了与学术界之外的更广泛的数据科学界的互动，并推动了对真实单词问题的使用。</li>
<li><strong>跨任务的适用性：</strong>在有监督的嵌入式学习中，情况可能更多，尽管并非所有无监督的技术都具有相同的综合水平。无论如何，依靠文本数据中不同类型的信息，各种各样非常多样化的NLP任务使这一问题成为突出问题。从多个任务中共同学习嵌入是一种受监督的方法可以解决这一挑战的有趣方式。</li>
<li><strong>标记语料库：</strong>大型标记语料库的有限可用性也是今后有监督方法的问题。这可能代表了未来几年无监督方法在有监督的表示学习中的真正优势。</li>
</ol>
<p><strong>注意：</strong>如果您发现这部分内容与上下文不符，建议您在仔细阅读本文中介绍的大部分技巧后再进行访问。</p>
<p>本节简要介绍了两种适用于文档嵌入的成熟技术：<em>词袋</em>  和*潜在Dirichlet 分配。</p>
<h2 id="传统经典方法"><a href="#传统经典方法" class="headerlink" title="传统经典方法"></a>传统经典方法</h2><h3 id="Bag-of-words"><a href="#Bag-of-words" class="headerlink" title="Bag-of-words"></a>Bag-of-words</h3><p> <strong>Bag-of-words</strong><br>在[Harris，1954]中提出的这种方法将文本表示为单词的包（<a href="https://en.wikipedia.org/wiki/Multiset" target="_blank" rel="noopener">Multiset</a>) （丢失语法和排序信息）。这是通过确定一组<em>n个</em>单词组成的，这些单词将构成映射支持的词汇表，并为词汇表中的每个单词分配唯一索引。然后，每个文档都由长度为<em>n</em> 的向量表示，其中第<em>i</em> 个条目包含单词<em>i</em> 在文档中出现的次数。</p>
<p><img src="1.png" alt="png"></p>
<p>图1：例句中的单词袋表示</p>
<p>例如，句子“<strong>dog eat dog world, baby</strong>!” （在清理标点之后）可能由550个长度的向量<em>v表示</em>（假设选择了550个单词的词汇），除以下条目外，其他地方均为零：</p>
<p>•$ V_{76}= 1$，因为词汇的第76个词是<em>world</em> 。       </p>
<p>• $ V_{200}= 2$，因为词汇的第200个单词是<em>dog</em> 。       </p>
<p>• $ V_{322}= 1$，因为第332个单词是 <em>eat</em>。       </p>
<p>• 单词中没有选择“ <em>baby</em> ”一词，因此在不输入向量的情况下其值为1。       </p>
<p>尽管它非常简单，除了单词出现频率之外，所有信息都丢失了，并且表示大小迅速增长以支持丰富的词汇的趋势，这种技术几乎在几十年中几乎全部用于NLP任务，并获得了巨大成功。即使近年来在文本的矢量表示方面取得了显着进展，但仍在使用此方法的常见细微变化（如下所述），如今，这种变化并不总是唯一的，因为它只是很快被超越的第一个基线。</p>
<p><strong>Bag-of-n-grams</strong><br>为了获得某些单词袋方法丢失的某些单词顺序信息，可以使用短单词序列（长度为2、3等）的频率（附加或替代）构造单词向量。自然地，对于<em>n = 1</em> ，词袋是此方法的一个私有案例。</p>
<p>对于“<strong>dog eat dog world, baby</strong>!” 一词对是”<strong>dog eat</strong>“，”<strong>eat dog </strong>“，”<strong>dog world </strong>“和”<strong>world baby</strong>“，词汇表由输入语料库中的所有连续单词对组成。</p>
<p> <img src="2.png" alt="2"></p>
<p>图2：“电影很棒”这句话的2-gram表示法</p>
<p>这种方法的一个主要缺点是词汇量大小对唯一单词数量的非线性依赖性，这对于大型语料库可能非常大。过滤技术通常用于减小词汇量。</p>
<p><strong>TF-IDF weighting</strong><br>在词袋环境中值得一提的最后一项相关技术是<a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" target="_blank" rel="noopener"><em>术语频率-逆文档频率</em></a>, 通常称为<strong>tf-idf</strong> 。此方法使用每个单词的<em>文档反向频率</em>（<strong>IDF</strong>）对上述单词（或<strong>n-gram</strong>）频率向量进行加权。单词的IDF就是语料库中文档数量除以该单词出现在其中的文档数量的对数。</p>
<script type="math/tex; mode=display">
DIF_i = log( \frac{ \# of documents  in corpus}{\# of documents in which word i appears in})</script><p>简而言之，TF术语随着单词出现的增加而增长，而IDF术语则随着单词的稀有性而增加。这是为了针对某些单词通常更多（或更少）频繁出现这一事实来调整频率得分。参见[<a href="http://pmcnamee.net/744/papers/SaltonBuckley.pdf" target="_blank" rel="noopener">Salton &amp; Buckley, 1988</a>]，可以全面了解术语加权方法。</p>
<h3 id="Latent-Dirichlet-allocation-LDA"><a href="#Latent-Dirichlet-allocation-LDA" class="headerlink" title="Latent Dirichlet allocation (LDA)"></a>Latent Dirichlet allocation (LDA)</h3><p>LDA是一种生成统计模型，它允许由未观察组解释一组观察结果，这些观察组解释了为什么某些数据部分相似。例如，如果观察是收集到文档中的单词，则假定每个文档都是少量主题的混合，并且每个单词的出现都可归因于文档的一个主题。</p>
<p>要将其与单词袋联系起来，可以将前一种方法视为文档在单词上的分布的简单化概率模型。然后，词袋向量代表我们对每个文档中非规范化词分布的最佳近似值；但是这里的文档是概率的基本单位，每个都是其唯一分布的单个样本。</p>
<p>因此，问题的关键在于通过添加潜在的（隐藏的）<em>K</em> 主题中间层，从这种简单的概率性文档模型（按单词分布）转移到更复杂的模型。</p>
<p> <img src="3.png" alt="3"></p>
<p>图3：概率模型从单词袋转移到LDA</p>
<p>现在，主题的特征是单词的分布，而文档则是主题的分布。文档的这种概率模型对应于文档的生成模型。假设预定数量的<em>K个</em>主题，要生成一<em>组长</em>度为<em>{Nᵢ}</em> 的<em>M个</em>文档，其中<em>Dir（）</em>表示<a href="https://en.wikipedia.org/wiki/Dirichlet_distribution" target="_blank" rel="noopener">Dirichlet分布</a>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. 对于每个主题 v ，采样一个单词分布φᵥ〜Dir  （β） 。</span><br><span class="line">2. 对于每个文档 i ，采样一个主题分布（或混合）θᵢ〜Dir  （ α  ） 。</span><br><span class="line">3. 为每个单词 j  生成长度为 Nᵢ的 文档 i ：</span><br><span class="line">   1. 为单词 j 进行服从于 Multinomial(θᵢ)分布的主题采样  .   </span><br><span class="line">   2. 对单词 j 服从于Multinomial(zᵢⱼ)分布 进行采样 。</span><br></pre></td></tr></table></figure></p>
<p>给定此模型和大量文档，问题就成为了推论之一，并且在推论过程中发现了上述各种分布的近似值。其中有θᵢ，每个文档<em>i</em> 的主题分布，维数<em>K的</em>向量。</p>
<p>因此，在推断模型的过程中，推断出维度为<em>K</em> 的向量空间，该向量空间以某种方式捕获了我们语料库中的主题或主题以及它们在文档中的共享方式。当然，可以将其视为这些文档的嵌入空间，并且-取决于<em>K</em> 的选择-其维数可以比基于词汇的维数小得多。</p>
<p>确实，虽然LDA的主要用例是无监督的主题/社区发现，但其他情况包括将所得的潜在主题空间用作文档语料库的嵌入空间。另外，请注意，其他主题建模技术（例如<a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization" target="_blank" rel="noopener">非负矩阵分解（NMF）</a> 和<a href="https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis" target="_blank" rel="noopener">概率潜在语义索引（PLSI））</a> 也可以类似的方式用于学习文档嵌入空间。</p>
<blockquote>
<p><strong>注意：</strong>从业者对概率主题模型的主要问题是稳定性。由于训练主题模型需要对概率分布进行采样，因此随着随机数生成器种子的变化，同一语料库的模型可能会有所不同。主题模型对相对较小的语料库更改的敏感性使此问题更加复杂。</p>
</blockquote>
<h2 id="无监督的文本嵌入"><a href="#无监督的文本嵌入" class="headerlink" title="无监督的文本嵌入"></a>无监督的文本嵌入</h2><p>本节中介绍的许多方法均受著名的词嵌入技术的启发，其中主要的方法是<em>word2vec</em> ，它们有时甚至是这些方法的直接概括。这些词嵌入技术有时也称为<em>神经概率语言模型</em>; 这些不是完全相同的术语，因为概率语言模型是<em>单词序列上的概率分布</em>，但是由于此方法是在[ <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="noopener">Bengio，2003</a>],中作为学习语言模型的一种方法而引入的，因此它们紧密相关。</p>
<p>即使假设您熟悉<em>word2vec</em> ，我仍然希望注意到此模型所做的重要假设，并且可能由这里审查的每个模型<em>（分布假设）推崇</em>。这是<a href="https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_hypothesis" target="_blank" rel="noopener">维基百科</a>的简短描述：</p>
<blockquote>
<p>语言学中的<strong>分布假设</strong>源于语言使用的<a href="https://en.wikipedia.org/w/index.php?title=Semantic_theory&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">语义理论</a>，即在相同上下文中使用和出现的单词往往具有相似的含义。<a href="https://en.wikipedia.org/wiki/J._R._Firth" target="_blank" rel="noopener">Firth</a>推广了“单词由其所拥有的公司来表征”的基本思想。分布假设是<a href="https://en.wikipedia.org/wiki/Statistical_semantics" target="_blank" rel="noopener">统计语义</a>.的基础。</p>
</blockquote>
<p>确实，很容易看到<em>word2vec</em> 和其他用于学习单词表示的自我监督方法都严重依赖此假设。毕竟，模型的症结在于，在学习从单词本身来预测单词上下文时（反之亦然）学习到的单词表示形式代表了捕获深层语义和句法概念和现象的向量空间。意思是，从单词的上下文中学习可以教会我们有关单词的含义和句法作用的知识。</p>
<p>在本节中，将介绍自我监督的文档表示学习，您将看到所有这些方法都维护单词的这一假设，并以某种方式将其扩展到较大的文本单元。</p>
<h3 id="n-gram-embeddings"><a href="#n-gram-embeddings" class="headerlink" title="n-gram embeddings"></a>n-gram embeddings</h3><p>[ <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">Mikolov et al，2013b</a> ]扩展了<em>word2vec</em> 的skip-gram模型，通过使用数据驱动方法识别大量短短语（作者专注于两个单词和三个单词的短语）来处理短短语。在<em>word2vec</em> 模型训练期间，将这些短语作为单独的标记。自然地，这不适合学习更长的短语-因为随着短语长度的增加，词汇量会激增-并且<em>势必不会泛化到看不见的短语</em> 及其遵循的方法。</p>
<p>莫西·哈祖姆（Moshe Hazoom）<a href="https://towardsdatascience.com/word2vec-for-phrases-learning-embeddings-for-more-than-one-word-727b6cf723cf" target="_blank" rel="noopener">对这种方法进行了精彩的实践回顾，</a>他的将其用于专注于金融领域的搜索引擎。</p>
<h3 id="Averaging-word-embeddings"><a href="#Averaging-word-embeddings" class="headerlink" title="Averaging word embeddings"></a>Averaging word embeddings</h3><p>有一种非常直观的方法可以从有意义的单词嵌入中构造文档嵌入：给定文档，对与文档单词对应的所有矢量执行一些矢量算术，以将它们汇总到同一嵌入空间中的单个矢量中；两种常见的汇总运算符是平均值和和。</p>
<p>在此基础上，您可能已经可以想象到，扩展<em>word2vec</em> 及其亲属的编码器-解码器体系结构以学习<em>如何</em>将单词向量组合到文档嵌入中可能会很有趣。遵循这一方法的方法属于这一类。</p>
<p>第二种可能性是使用固定的（不可学习的）运算符进行矢量汇总（例如求平均），并使用旨在产生丰富文档嵌入的学习目标来学习上一层中的单词嵌入；一个常见的示例是使用句子来预测上下文句子。因此，这里的主要优点是优化了词嵌入，以平均化成文档表示形式。</p>
<p> <img src="4.png" alt="4"></p>
<p>图4：来自[ <a href="https://arxiv.org/pdf/1606.04640.pdf" target="_blank" rel="noopener">Kenter等，2016</a> ]的暹罗CBOW网络架构</p>
<p>[ <a href="https://arxiv.org/pdf/1606.04640.pdf" target="_blank" rel="noopener">Kenter等人，2016</a> ]做到了这一点，即使用平均单词向量的简单神经网络，通过给定句子表示形式预测周围的句子，从而学习单词嵌入。他们将结果与平均的<em>word2vec</em> 向量和<em>跳思想</em>向量进行了比较（请参见下面的相应小节）。[ <a href="https://www.aclweb.org/anthology/N16-1162" target="_blank" rel="noopener">Hill等，2016</a> ]比较了许多方法，包括训练CBOW和skip-gram词嵌入，同时优化句子表示（此处使用词向量的逐元素加法）。[ <a href="http://wwwusers.di.uniroma1.it/~navigli/pubs/KBS_Sinoaraetal_2019.pdf" target="_blank" rel="noopener">Sinoara等人，2019</a> ]还提出了将单词嵌入向量和其他知识源（例如单词感知向量）直接嵌入其质心以表示文档的方法。</p>
<p>最后，[ <a href="https://pdfs.semanticscholar.org/3fc9/7768dc0b36449ec377d6a4cad8827908d5b4.pdf" target="_blank" rel="noopener">Arora等人，2016年</a> ]进一步表明，当通过两个小变化进行增强时，此方法是一种简单但难以击败的基线：（1）使用平滑的逆频率加权方案，以及（2）消除常见的词向量的语篇成分；该组件是使用PCA找到的，它被用作最常用语的更正词，大概与语法有关。作者提供了一个<a href="https://github.com/peter3125/sentence2vec" target="_blank" rel="noopener">Python实现</a>。</p>
<blockquote>
<p><strong>注意：</strong>当查看基于注意力的机器翻译模型时，也许可以找到正确平均的单词“嵌入”功能的另一种证明。单向解码器RNN获得先前翻译的单词作为输入，不仅是要翻译的当前单词的“嵌入”（即，来自编码器RNN的双向激活），还包括周围单词的翻译。这些以加权的方式平均为上下文向量。据教导，这种加权平均能够从编码器网络的激活中维护复杂的成分和与顺序相关的信息（回想一下，这些不是像我们的情况那样不是孤立的嵌入;每个嵌入都包含前一个/后续单词的上下文）。</p>
</blockquote>
<h3 id="Sent2Vec"><a href="#Sent2Vec" class="headerlink" title="Sent2Vec"></a>Sent2Vec</h3><p>在[ <a href="https://aclweb.org/anthology/N18-1049" target="_blank" rel="noopener">Pagliardini et al，2017</a> ]和[ <a href="https://www.aclweb.org/anthology/N19-1098" target="_blank" rel="noopener">Gupta et al，2019</a> ]中提出（包括<a href="https://github.com/epfml/sent2vec" target="_blank" rel="noopener">基于C ++的官方Python实现</a>），该技术很大程度上是上述两种方法的组合：<em>word2vec</em> 的经典CBOW模型都得到了扩展包括单词n-gram <em>并</em>适用于优化单词（和n-grams）嵌入，以便对其求平均以产生文档向量。</p>
<p> <img src="5.png" alt="5"></p>
<p>图5：sent2vec可以看作是fastText的无监督版本</p>
<p>另外，删除了输入子采样的过程，而是将整个句子视为上下文。这意味着</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">（a）放弃使用频繁的单词二次采样（以防止生成n元语法特征）</span><br><span class="line">（b）放弃word2vec 使用的动态上下文窗口：考虑整个句子作为上下文窗口，而不是在1和当前句子的长度之间均匀采样每个子采样单词的上下文窗口大小。</span><br></pre></td></tr></table></figure>
<p>想到另一种方式<em>sent2vec</em> 是作为一种无监督的版本<em>fastText</em> ，其中整个句子是上下文和可能的类标签都是词汇。巧合的是，[ <a href="https://link.springer.com/article/10.1186/s12859-018-2496-4" target="_blank" rel="noopener">Agibetov等人，2018</a> ]感知使用比较多层的性能<em>sent2vec</em> 向量作为特征到的<em>fastText</em> ，针对生物医学句子分类的任务。</p>
<h3 id="Paragraph-vectors-doc2vec"><a href="#Paragraph-vectors-doc2vec" class="headerlink" title="Paragraph vectors (doc2vec)"></a>Paragraph vectors (doc2vec)</h3><p>有时称为<em>doc2vec</em> ，此方法在[ <a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" target="_blank" rel="noopener">Le＆Mikolov，2014</a> ]中提出，可能是首次尝试推广<em>word2vec</em> 以使其与单词序列配合使用。作者介绍了<em>段落向量</em>模型的两个变体：<em>分布式内存</em>和<em>分布式词袋。</em></p>
<h3 id="Paragraph-Vectors-Distributed-Memory-PV-DM"><a href="#Paragraph-Vectors-Distributed-Memory-PV-DM" class="headerlink" title="Paragraph Vectors: Distributed Memory (PV-DM)"></a>Paragraph Vectors: Distributed Memory (PV-DM)</h3><p>PV-DM模型通过添加旨在捕捉段落主题或输入内容的内存向量来增强标准的编码器-解码器模型。这里的训练任务与<em>连续单词</em>的训练非常类似; 一个单词要根据其上下文进行预测。在这种情况下，上下文单词是前面的单词，而不是段落周围的单词。</p>
<p> <img src="6.png" alt="6"></p>
<p>图6：段落向量的分布式内存模型（PV-DM）</p>
<p>为了实现这一点，每个段落都映射到一个唯一的向量，该向量由词汇表中矩阵中的一列表示（用<em>D</em> 表示）。上下文是固定长度的，并从段落上的滑动窗口中采样。段落向量在同一段落生成的所有上下文之间共享，但不跨段落共享。自然，词嵌入是全局的，并且可以使用经过预训练的词嵌入（请参见下面的<em>实现和增强</em>）。</p>
<p>与<em>word2vec中一样</em>，向量必须以某种方式汇总为单个向量。但是与<em>word2vec</em> 不同，作者在实验中使用串联。请注意，这将保留订单信息。与<em>word2vec</em> 相似，在此汇总的矢量表示上使用了一个简单的softmax分类器（在这种情况下，实际上是分层的softmax）来预测任务输出。使用随机梯度下降法并通过反向传播获得梯度，以标准方式进行训练。</p>
<p>请注意，只有训练语料库中的段落才具有来自<em>D</em> 的列向量。在预测时，需要执行一个推理步骤来计算新段落的段落向量：文档向量是随机初始化的。然后，重复地从新文档中选择一个随机词，并使用梯度下降来调整输入到隐藏层的权重，以使所选词的softmax概率最大化，而隐藏到softmax的输出权重为固定。这导致将新文档表示为训练语料库文档向量（即<em>D的</em>列）的混合物，自然而然地位于文档嵌入空间中。</p>
<p><strong>段落向量：分布式词袋（PV-DBOW）</strong><br>的第二种变体，尽管其名称如此，也许与<em>word2vec</em> 的<em>skip-gram</em> 体系结构相似。分类任务是仅使用段落向量来预测单个上下文词。在随机梯度下降的每次迭代中，对文本窗口进行采样，然后从该窗口中采样单个随机词，从而形成以下分类任务。</p>
<p> <img src="7.png" alt="7"></p>
<p>图7：段落向量的分布式词袋模型（PV-DBOW）</p>
<p>除不与段落向量一起共同学习单词向量的事实外，训练在其他方面相似。这使得PV-DBOW变体的内存和运行时性能都更好。</p>
<blockquote>
<p><strong>注意：</strong>在<a href="https://radimrehurek.com/gensim/models/doc2vec.html" target="_blank" rel="noopener">其Gensim实现中</a>，PV-DBOW默认情况下使用随机初始化的词嵌入；如果dbow_words设置为1，则在运行dbow之前运行一次skip-gram步骤以更新单词嵌入。[ <a href="https://arxiv.org/pdf/1607.05368.pdf" target="_blank" rel="noopener">Lau＆Baldwin，2016</a> ]认为，尽管dbow在理论上可以与随机词嵌入一起使用，但这在他们检查的任务中会严重降低性能。</p>
</blockquote>
<p>直观的解释可以追溯到模型的目标函数，该函数的目的是使文档嵌入与其组成的单词嵌入之间的点积最大化：如果单词嵌入是随机分布的，则优化文档嵌入使其更接近于变得更加困难。其更关键的内容词。</p>
<p><strong>应用</strong><br>[Le＆Mikolov，2014]演示了<em>段落向量</em>在多个文本分类和情感分析任务上的使用，而[Dai等，2015]在文档相似性任务和[Lau＆Baldwin， [2016年]以论坛问题重复任务和<a href="http://ixa2.si.ehu.es/stswiki/index.php/Main_Page" target="_blank" rel="noopener"><em>语义文本相似性（STS）SemEval</em></a>共享任务为<a href="http://ixa2.si.ehu.es/stswiki/index.php/Main_Page" target="_blank" rel="noopener"><em>基准</em></a>。后面的两篇论文都对该方法进行了扩展评估（前者侧重于PV-DBOW变体），将其与其他几种方法进行了比较，并提供了实用建议（后者<a href="https://github.com/jhlau/doc2vec" target="_blank" rel="noopener">包括代码</a>）。</p>
<p>该方法具有<a href="https://radimrehurek.com/gensim/models/doc2vec.html" target="_blank" rel="noopener">gensim包的一部分的Python实现</a>和<a href="https://github.com/inejc/paragraph-vectors" target="_blank" rel="noopener">PyTorch实现</a>。再次，[ <a href="https://arxiv.org/pdf/1607.05368.pdf" target="_blank" rel="noopener">Lau＆Baldwin，2016年</a> ]还<a href="https://github.com/jhlau/doc2vec" target="_blank" rel="noopener">提供了用于检查的代码</a>。</p>
<p>最后，提出了对该方法的各种增强。例如，[ <a href="https://arxiv.org/abs/1512.08183" target="_blank" rel="noopener">Li等人，2016年</a> ]将该方法扩展为还包含n元语法特征，而[Thongtan＆Phienthrakul，2019年]建议在计算嵌入投影时也使用余弦相似度代替点积（也提供<a href="https://github.com/tanthongtan/dv-cosine" target="_blank" rel="noopener">Java实现</a>）。</p>
<h3 id="Doc2VecC"><a href="#Doc2VecC" class="headerlink" title="Doc2VecC"></a>Doc2VecC</h3><p>[ <a href="https://arxiv.org/pdf/1707.02377.pdf" target="_blank" rel="noopener">Chen，2017</a> ]提出了一种有趣的方法，该方法受段向量方法（PV-DM）的分布式存储模型以及平均词嵌入来表示文档的方法的启发。</p>
<p> <img src="8.png" alt="8"></p>
<p>图8：Doc2VecC模型的架构</p>
<p>类似于<em>段落向量</em>，<em>Doc2VecC</em> （通过损坏的文档向量的缩写）由输入层，投影层和输出层组成，以预测目标单词（在上例中为<strong>“ceremony”</strong>）。相邻单词的嵌入(e.g. <strong>“opening”, “for”, “the”</strong>)提供局部上下文，而整个文档的矢量表示（以灰色显示）用作全局上下文。与直接针对每个文档学习唯一矢量的<em>段落矢量</em>相反，<em>Doc2VecC</em> 将每个文档表示为从文档中随机采样的单词嵌入的平均值（例如，位置<em>p</em> 处的<strong>“performance”</strong>，位置<em>q</em> 处的<strong>“praised”</strong> 和<strong>“brazil”</strong>在位置<em>r</em> ）。</p>
<p>另外，作者选择通过随机删除大部分单词来破坏原始文档，从而仅对其余单词的嵌入进行平均，从而代表文档。这种破坏机制可以在训练期间加快速度，因为它大大减少了反向传播中要更新的参数数量。作者还展示了它是如何引入一种特殊形式的正则化的，他们认为这种形式可导致观察到的性能提高，以情感分析任务，文档分类任务和语义相关性任务为基准，而不是大量的最新技术。文档嵌入技术。</p>
<p>可以在<a href="https://github.com/mchen24/iclr2017" target="_blank" rel="noopener">公共Github存储库中</a>找到基于C语言的开源实现，用于重现本文中的实验。</p>
<p>[ <a href="https://www.aclweb.org/anthology/N16-1162" target="_blank" rel="noopener">Hill等人，2016</a> ]还对<em>Skip-thought</em>模型（请参见以下小节）应用了破坏或增加噪声的一般思想，以增加文档的嵌入学习过程以产生更强大的嵌入空间。创建其顺序降噪自动编码器（SDAE）模型。</p>
<h3 id="Skip-thought-vectors"><a href="#Skip-thought-vectors" class="headerlink" title="Skip-thought vectors"></a>Skip-thought vectors</h3><p>这是在[ <a href="https://arxiv.org/abs/1506.06726" target="_blank" rel="noopener">Kiros等，2015</a> ]中提出的，这是对<em>word2vec</em> 进行泛化的另一种早期尝试，并且已与<a href="https://github.com/ryankiros/skip-thoughts" target="_blank" rel="noopener">官方的纯Python实现一起发布</a>（并且最近还夸耀了<a href="https://github.com/sanyam5/skip-thoughts" target="_blank" rel="noopener">PyTorch</a>和<a href="https://github.com/tensorflow/models/tree/master/research/skip_thoughts" target="_blank" rel="noopener">TensorFlow的实现</a>）。</p>
<p>但是，这以另一种直观的方式扩展了<em>word2vec</em> （尤其是<em>skip-gram</em> 体系结构）：基本单元现在是句子，并且已编码的句子用于预测其周围的句子。使用在上述任务上训练的编码器-解码器模型学习矢量表示。作者使用具有GRU激活功能的RNN编码器和具有条件GRU的RNN解码器。训练了两个不同的解码器以用于上一个和下一个句子。</p>
<p> <img src="9.png" alt="9"></p>
<p>图9：跳过思路模型。给定一个连续句子元组，对句子$s<em>i$进行编码，并尝试重建上一个句子$s</em>{i- 1}$和下一个句子$s_{i + 1}$</p>
<p>的<strong>词汇扩展跳</strong>字<em>思维</em>编码器使用单词嵌入层，将输入句子中的每个单词转换为对应的单词嵌入，从而有效地将输入句子转换为单词嵌入序列。该嵌入层也与两个解码器共享。</p>
<p> <img src="10.png" alt="10"></p>
<p>图10：在Skip-thought vectors中，句子sᵢ由编码器编码；两个解码器以编码器输出hᵢ的隐藏表示为条件，以预测$s<em>{i - 1}$和$s</em>{i + 1}$[摘自<a href="https://sourcediving.com/building-recipe-skill-representations-using-skip-thought-vectors-8a6e4c38ae6c" target="_blank" rel="noopener">Ammar Zaher的帖子</a> ]</p>
<p>但是，作者仅使用了20,000个单词的小词汇量，因此在执行各种任务期间可能会遇到许多看不见的单词。为了克服这个问题，通过解决矩阵<em>W</em> 参数化的非正规<em>L2</em> 线性回归损失，可以从在更大的词汇量上训练的词嵌入空间（例如<em>word2vec</em> ）到<em>Skip-thought</em>模型的词嵌入空间中学习映射。此映射。</p>
<p><strong>应用程序</strong><br>作者演示了使用<em>Skip-thought的</em>向量进行语义相关性，释义检测，图像句子排名，问题类型分类以及四个情感和主观性数据集。[ <a href="http://arno.uvt.nl/show.cgi%3Ffid%3D146003" target="_blank" rel="noopener">Broere，2017</a> ]进一步研究了<em>跳思维</em>句子表示的句法属性，方法是对它们进行逻辑回归训练以预测POS标签和依赖关系。</p>
<p>[ <a href="https://arxiv.org/abs/1706.03146" target="_blank" rel="noopener">Tang等，2017a</a> ]提出了一种邻域方法，用于<em>Skip-thought</em>，丢弃排序信息并使用单个解码器预测前一句话和下一句话。[ <a href="https://www.groundai.com/project/trimming-and-improving-skip-thought-vectors/1" target="_blank" rel="noopener">Tang et al，2017b</a> ]扩展了这项检查，以提出对模型的三个增强，他们声称使用更快更轻的模型可以提供可比的性能：</p>
<p>（1）仅学习解码下一个句子</p>
<p>（2）添加<em>avg  编码器和解码器之间的</em>最大*连接层（作为一种允许进行非线性非参数特征工程的方法）</p>
<p>（3）执行良好的词嵌入初始化。最后，[ <a href="https://arxiv.org/pdf/1611.07897.pdf" target="_blank" rel="noopener">Gan等，2016</a> ]在广泛的应用中，使用基于分层CNN-LSTM的编码器而非仅基于RNN的编码器<a href="https://arxiv.org/pdf/1611.07897.pdf" target="_blank" rel="noopener">，</a>采用了相同的方法。</p>
<p>在[ <a href="https://openreview.net/pdf%3Fid%3DH1a37GWCZ" target="_blank" rel="noopener">Lee＆Park，2018</a> ]中提出的另一种变体是通过基于文档结构为每个目标句子选择整个文档中有影响力的句子来学习句子嵌入的，从而使用元数据或文本样式识别句子的依存关系。此外，[ <a href="https://www.aclweb.org/anthology/N16-1162" target="_blank" rel="noopener">Hill等人，2016年</a> ]提出了<em>顺序降噪自动编码器（SDAE）</em>模型，这是一种<em>跳</em>变<em>思想</em>的变体，其中输入数据根据某些噪声函数而被破坏，并且训练该模型以从破坏的数据中恢复原始数据。 。</p>
<p>有关进一步的非学术阅读<em>跳跃思维</em> 模式，<a href="http://sanyam5.github.io/my-thoughts-on-skip-thoughts/" target="_blank" rel="noopener">Sanyam Agarwa给出了他的博客的方法有很大的详细介绍</a></p>
<h3 id="FastSent"><a href="#FastSent" class="headerlink" title="FastSent"></a>FastSent</h3><p>[ <a href="https://www.aclweb.org/anthology/N16-1162" target="_blank" rel="noopener">Hill等人，2016年</a> ]提出了一种关于<em>跳跃思维</em>模型的明显简化的变体。<em>FastSent</em> 是一个简单的加法（对数双线性）语句模型，旨在利用相同的信号，但计算成本却低得多。给定某些上下文句子的BOW表示，该模型仅预测相邻句子（也表示为BOW）。更正式地说，<em>FastSent</em> 为模型词汇表中的每个单词<em>w</em> 学习源uᵂ和目标vᵂ嵌入。对于训练例如$S<em>{i- 1}$，$S</em>{i}$，连续句子$S<em>{i+1}$，$S</em>{i- 1}$被表示为它的源的嵌入的总和$s_i = \sum u^w $超过$w∈S_i$ 。$φ（s_i，v^w）$超过</p>
<p>$w∈ S<em>{i- 1}∪S</em>{i + 1}$，其中φ是SOFTMAX功能。本文附带了<a href="https://github.com/fh295/SentenceRepresentation" target="_blank" rel="noopener">一个官方的Python实现</a>。</p>
<h3 id="Quick-thought-vectors"><a href="#Quick-thought-vectors" class="headerlink" title="Quick-thought vectors"></a>Quick-thought vectors</h3><p>[ <a href="https://arxiv.org/pdf/1803.02893.pdf" target="_blank" rel="noopener">Logeswaran＆Lee，2018</a> ]将文件嵌入任务（即预测句子出现上下文的问题）重新设计为监督分类问题（参见图12b），而不是先前方法的预测任务（参见图12a）。</p>
<p> <img src="11.png" alt="11"></p>
<p>图11：快速思考问题的表述（b）与跳过思考方法（a）的对比</p>
<p>要点是使用当前句子的含义来预测相邻句子的含义，其中含义由从编码函数计算出的句子的嵌入表示；注意，这里学习了两个编码器：<em>f</em> 代表输入语句，<em>g</em> 代表候选项。给定一个输入语句，它由编码器（在这种情况下为RNN）进行编码，但是模型没有生成目标语句，而是从一组候选语句中选择了正确的目标语句。候选集是根据有效的上下文句子（基本事实）和许多其他非上下文句子构建的。最后，构造的训练目标最大程度地为训练数据中的每个句子标识了正确的上下文句子。将以前的句子预测公式看作是从所有可能的句子中选择一个句子，这种新方法可以看作是对预测问题的判别近似。</p>
<p>作者评估了他们在各种文本分类，释义识别和语义相关性任务上的方法，并提供<a href="https://github.com/lajanugen/S2V" target="_blank" rel="noopener">了官方的Python实现</a>。</p>
<h3 id="Word-Mover’s-Embedding-WME"><a href="#Word-Mover’s-Embedding-WME" class="headerlink" title="Word Mover’s Embedding (WME)"></a>Word Mover’s Embedding (WME)</h3><p>来自IBM研究的一种非常新的方法是在[ <a href="https://arxiv.org/pdf/1811.01713v1.pdf" target="_blank" rel="noopener">Wu et al，2018b</a> ]中提出的<em>Word Mover的嵌入</em>（WME）。<a href="https://github.com/IBM/WordMoversEmbeddings" target="_blank" rel="noopener">提供了一个基于C的官方官方Python封装实现</a>。</p>
<p>[ <a href="http://proceedings.mlr.press/v37/kusnerb15.pdf" target="_blank" rel="noopener">Kushner et al，2015</a> ]提出了世界<em>移动距离</em>（WMD）；这测量了两个文本文档之间的差异，这是一个文档的嵌入单词<strong>在嵌入空间中</strong>需要“移动” 才能到达另一文档的嵌入单词的最小距离（参见图13a）。另外，[ <a href="https://arxiv.org/pdf/1802.04956.pdf" target="_blank" rel="noopener">Wu等，2018a</a> ]提出了D2KE（到核和嵌入的距离），这是一种从给定距离函数推导正定核的通用方法。</p>
<p> <img src="12.png" alt="12"></p>
<p>图12：WMD与WME的对比。（a）WMD测量两个文档<em>x</em> 和y 之间的距离，而（b）WME近似于从WMD导出的带有一组随机文档kernel的核。</p>
<p>WME基于三个组件来学习长度可变的文本的连续矢量表示形式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 以无人监督的方式学习高质量词嵌入的能力（例如，使用*word2vec* ）。</span><br><span class="line">2. 使用WMD基于所述嵌入为文档构造距离度量的能力。</span><br><span class="line">3. 使用D2KE从给定的距离函数导出正定核的能力。</span><br></pre></td></tr></table></figure>
<p>使用这三个组件，将应用以下方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. 使用D2KE，通过给定分布中*单词移动器的距离*（WMD）到随机文档given的无限维特征图，构造正定*单词移动器的内核（WMK）。由于使用了WMD，因此特征图考虑了预训练单词嵌入所给出的语义空间中文档之间各个单词的对齐方式（请参见图12b）。</span><br><span class="line">2. 基于该内核，通过该内核的随机特征近似推导嵌入的文档，其内积近似于精确的内核计算。</span><br></pre></td></tr></table></figure>
<p>该框架是可扩展的，因为它的两个构件<em>word2vec</em> 和WMD可以用其他技术代替，例如<em>GloVe</em> （用于词嵌入）或S-WMD（用于将词嵌入空间转换为文档距离度量）。</p>
<p>作者在9个真实世界中的文本分类任务和22个文本相似性任务上对WME进行了评估，并证明了WME与其他最新技术始终如一地匹配，甚至优于其他最新技术。</p>
<h3 id="Sentence-BERT-SBERT"><a href="#Sentence-BERT-SBERT" class="headerlink" title="Sentence-BERT (SBERT)"></a>Sentence-BERT (SBERT)</h3><p>NLP的2018年以Transformer的兴起为标志，最新的神经语言模型受到[ <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Vaswani et al 2017</a> ]中提出的Transformer模型的启发-序列模型同时消除了卷积和重复发生，而是将注意力转移到序列表示中。这个蓬勃发展的系列包括BERT（及其扩展名），GPT（1和2）以及XL风味的Transformer。</p>
<p>这些模型生成输入令牌（通常为子单词单元）的上下文嵌入，每个令牌都注入了其邻域的信息，但并非旨在为输入序列生成丰富的嵌入空间。BERT甚至有一个特殊的[CLS]令牌，其输出嵌入用于分类任务，但对于其他任务而言，输入序列的嵌入仍然很差。[ <a href="https://arxiv.org/pdf/1908.10084.pdf" target="_blank" rel="noopener">Reimers＆Gurevych，2019年</a> ]</p>
<p><em>Sentence-BERT</em> ，在[ <a href="https://arxiv.org/pdf/1908.10084.pdf" target="_blank" rel="noopener">Reimers＆Gurevych，2019</a> ]中提出，并附带<a href="https://github.com/UKPLab/sentence-transformers" target="_blank" rel="noopener">一个Python实现</a>，旨在通过使用暹罗和三元组网络结构来派生可以使用余弦相似度进行比较的语义有意义的句子嵌入，以适应BERT体系结构（请参见Fifure 14）。</p>
<p><img src="13.png" alt="13"></p>
<p>图14：针对分类目标（左）和推理（右）的训练中的SBERT架构</p>
<h2 id="有监督的向量嵌入"><a href="#有监督的向量嵌入" class="headerlink" title="有监督的向量嵌入"></a>有监督的向量嵌入</h2><p><code>上一节中介绍的无监督方法使我们可以从大型未标记的语料库中学习有用的表示形式。这种方法并非自然语言处理所独有，而是通过设计学习目标来学习学习表示形式，这些学习目标利用了数据中可自由使用的标签。因此，这些方法的强度和鲁棒性不仅在很大程度上取决于学习框架，而且还取决于人工设计的学习目标要求或带来对有意义的特征或知识的学习的良好程度，这些特征或知识在各种下游任务中将是有用的。例如，我们希望通过单词和文档嵌入空间可以很好地捕获语义和句法信息。</code></p>
<p><code>学习数据有意义的表示法（在我们的情况下是单词序列）的对比方法是利用显式标签（几乎总是由人类注释者以某种方式生成）。在这里，与各种任务的相关性取决于显式任务和用于最终应用程序的标签的接近程度，并且再次取决于此任务带来了对通用特性和知识的学习程度。</code></p>
<p><code>我们将看到有监督的方法，从直接利用特定标记任务来学习表示形式的方法，到重组任务或从中提取新标记任务以引发更好表示的方法。</code></p>
<p><strong>通过标签数据学习文本嵌入</strong></p>
<p>已经进行了各种尝试来使用标记的或结构化的数据来学习句子表示。具体来说，[ <a href="https://www.aclweb.org/anthology/D14-1179" target="_blank" rel="noopener">Cho等，2014a</a> ]和[ <a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">Sutskever等，2014</a> ]可能是首次尝试使用编码器/解码器方法来显式学习带有标记数据的句子/短语嵌入；第一个使用<em>Europarl</em> （统计机器翻译的平行短语语料库），第二个使用WMT-14数据集中的英语到法语的翻译任务。[ <a href="https://arxiv.org/pdf/1511.08198.pdf" target="_blank" rel="noopener">Wieting et al，2015</a> ]和[Wieting＆Gimpel，2017] 提出了另一种此类显着尝试，其中共同学习了单词嵌入及其在文档嵌入中的映射，以最大程度地减少了复述对之间的余弦相似度（来自<a href="http://paraphrase.org/%23/#/" target="_blank" rel="noopener">PPDB数据集</a>） 。[ <a href="https://arxiv.org/pdf/1504.00548.pdf" target="_blank" rel="noopener">Hill等，2015</a> ]训练了神经语言模型，以将字典定义映射到由这些定义定义的单词的预训练单词嵌入。最后，[ <a href="https://www.aclweb.org/anthology/D17-1070.pdf" target="_blank" rel="noopener">Conneau等，2017</a> ]在斯坦福大学自然语言推理任务上训练了各种体系结构的NN编码器（请参见图16）。</p>
<p> <img src="14.png" alt="14"></p>
<p>图15：通用的NLI训练方案</p>
<h3 id="文档相似性的上下文嵌入"><a href="#文档相似性的上下文嵌入" class="headerlink" title="文档相似性的上下文嵌入"></a>文档相似性的上下文嵌入</h3><p>上述方法的一种特定情况是由文档相似性驱动的。[ <a href="https://www.aclweb.org/anthology/P16-1036" target="_blank" rel="noopener">Das等，2016</a> ]展示了文档嵌入，这些嵌入是通过针对社区Q / A的暹罗网络使两个文档之间的相似度最大化的。（见图17）</p>
<p> <img src="15.png" alt="png"></p>
<p>图16：SCQA网络由重复的卷积，最大池和ReLU层以及一个完全连接的层组成。权重W1到W5在子网之间共享。</p>
<p>同样，[ <a href="https://www.aclweb.org/anthology/K17-1027" target="_blank" rel="noopener">Nicosia＆Moschitti，2017</a> ]使用暹罗网络在学习二进制文本相似性的同时产生单词表示，同时考虑相同类别中的示例相似。（参见图18）</p>
<p> <img src="16.png" alt="16"></p>
<p>图17：[ <a href="https://www.aclweb.org/anthology/K17-1027" target="_blank" rel="noopener">Nicosia＆Moschitti，2017</a> ] 中的暹罗网络架构。每个句子的单词嵌入由3个双向GRU的堆栈使用。两个网络分支共享参数权重。</p>
<p>跨语言降<strong>秩岭回归（Cr5）</strong>[Josifoski et al，2019]引入了一种方法，用于将以任何语言编写的文档嵌入到一个独立于语言的向量空间中。这是通过训练基于岭回归的分类器来完成的，该分类器使用特定于语言的词袋功能来预测给定文档所涉及的概念。当将学习的权重矩阵约束为低等级时，作者表明可以将其作为因素来获得从特定于语言的词袋到独立于语言的嵌入的期望映射。<a href="https://github.com/epfl-dlab/Cr5" target="_blank" rel="noopener">提供了一个官方的Python实现</a>。</p>
<h3 id="特定于任务的监督文档嵌入"><a href="#特定于任务的监督文档嵌入" class="headerlink" title="特定于任务的监督文档嵌入"></a>特定于任务的监督文档嵌入</h3><p>一种用于生成文档嵌入的常见监督方法是使用各种神经网络体系结构，学习将单词向量映射到文档向量的合成运算符；它们被传递给受监督的任务，并依赖于类标签，以便在合成权重之间反向传播（请参见图19）。</p>
<p>因此，网络的几乎所有隐藏层都可以被视为产生输入文档的向量嵌入，其中直到该层的网络前缀都是从单词向量到嵌入空间的学习映射。在[ <a href="https://arxiv.org/pdf/1511.08198.pdf" target="_blank" rel="noopener">Wieting等人，2015</a> ]中可以找到对基于单词向量和监督学习任务的学习句子向量的不同方法的严格检查。</p>
<p> <img src="17.png" alt="17"></p>
<p>图18：神经网络隐式学习将单词嵌入序列映射到文档嵌入</p>
<p>请注意，虽然所使用的单词嵌入可以预先生成并且与任务无关（至少在一定程度上），但从它们中学到的文档嵌入映射是特定于任务的。尽管这些方法对相关任务很有用，但至少在理论上，与无监督方法相比，此方法势必不那么健壮和通用。[ <a href="https://arxiv.org/pdf/1506.06726.pdf" target="_blank" rel="noopener">Kiros等，2015</a> ]</p>
<p>值得注意的用途包括使用RNN进行情感分类[Socher等，2013]，使用CNN进行各种文本分类[Kalchbrenner等，2014] [Kim，2014]以及使用递归卷积神经网络进行机器翻译和文本分类[Cho等]等，2014a，2014b] [Zhao等，2015]。</p>
<ul>
<li><p><strong>GPT</strong><br>[ <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">Radford等人，2018</a> ] <a href="https://openai.com/blog/language-unsupervised/" target="_blank" rel="noopener">提出了一种</a><a href="https://openai.com/blog/language-unsupervised/" target="_blank" rel="noopener"><em>生成式预训练</em></a><a href="https://openai.com/blog/language-unsupervised/" target="_blank" rel="noopener">（GPT）方法</a>（<a href="https://github.com/openai/finetune-transformer-lm" target="_blank" rel="noopener">伴随Python实现</a>），使用[ <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Vaswani等人2017</a> ]中介绍的转换模型，将无监督和有监督的表示学习结合起来，学习无监督语言对未标记的语料库进行建模，然后使用监督数据分别微调其在每个任务中的使用。<a href="https://openai.com/blog/better-language-models/" target="_blank" rel="noopener">他们随后</a>在[ <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener">Radford et al，2019</a> ]中<a href="https://openai.com/blog/better-language-models/" target="_blank" rel="noopener">介绍了GPT-2</a>，重点是加强他们工作中的无监督学习部分，并再次<a href="https://github.com/openai/gpt-2" target="_blank" rel="noopener">发布了正式的Python实现</a>。</p>
</li>
<li><p><strong>深度语义相似性模型（DSSM）</strong><br><a href="https://www.microsoft.com/en-us/research/project/dssm/" target="_blank" rel="noopener">作为Microsoft研究项目</a>，DSSM是一种深度神经网络建模技术，用于表示连续语义空间中的文本字符串并为两个文本字符串之间的语义相似性建模（请参见图20）。</p>
<p><img src="19.png" alt="19"></p>
</li>
</ul>
<p>图19：DSSM神经网络的架构</p>
<p>除其他应用程序外，DSSM还用于开发潜在语义模型，该模型将不同类型的实体（例如查询和文档）投影到公共的低维语义空间中，以用于各种机器学习任务，例如排名和分类。例如，[ <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf" target="_blank" rel="noopener">Huang et al，2013</a> ]使用它将查询和文档投影到一个公共的低维空间，在该空间中，给定查询的文档的相关性被计算为它们之间的距离。</p>
<p>实现包括<a href="https://kishorepv.github.io/DSSM/" target="_blank" rel="noopener">TensorFlow</a>，<a href="https://github.com/airalcorn2/Deep-Semantic-Similarity-Model" target="_blank" rel="noopener">Keras</a>和<a href="https://github.com/nishnik/Deep-Semantic-Similarity-Model-PyTorch" target="_blank" rel="noopener">2个PyTorch </a><a href="https://github.com/moinnadeem/CDSSM" target="_blank" rel="noopener">变化</a>。</p>
<h3 id="联合学习句子表示"><a href="#联合学习句子表示" class="headerlink" title="联合学习句子表示"></a>联合学习句子表示</h3><p>[ <a href="https://arxiv.org/pdf/1810.00681v1.pdf" target="_blank" rel="noopener">Ahmad et al，2018</a> ]建议从多个文本分类任务中共同学习句子表示，并将它们与预训练的单词级和句子级编码器结合使用，可以得到健壮的句子表示，可用于迁移学习</p>
<p> <img src="18.png" alt="18"></p>
<p>图20：使用辅助任务共同学习句子嵌入</p>
<p>[ <a href="https://www.semanticscholar.org/paper/Learning-Sentence-Embeddings-with-Auxiliary-Tasks-Yu-Jiang/2d38f7aab07d4435b2110602db4138ef20da4cc0" target="_blank" rel="noopener">Yu＆Jiang，2016</a> ]类似地表明，使用两个辅助任务来帮助诱导句子嵌入在情感分类中跨域的工作很有效，与情感分类器本身一起共同学习该句子的嵌入（图21）。</p>
<p>在[ <a href="https://arxiv.org/pdf/1803.11175.pdf" target="_blank" rel="noopener">Cer等人，2018a</a> ]和[ <a href="https://www.aclweb.org/anthology/D18-2029/" target="_blank" rel="noopener">Cer等人，2018b</a> ]中提出的</p>
<p><strong>通用句子编码器</strong>，并伴随<a href="https://tfhub.dev/google/universal-sentence-encoder/2" target="_blank" rel="noopener">着TensorFlow实现</a>，该方法实际上包括两种可能的句子表示学习模型：<em>Transformer</em> 模型和<em>Deep Averaging Network（DAN） ）</em>模型（请参见图22）。两者都旨在允许多任务学习，并且支持的任务包括（1）作为无监督学习的<em>基调思维</em>任务；（2）对话输入响应任务，用于包含已解析的对话数据；（3）用于监督数据训练的分类任务（请参阅前面的小节）。作者专注于具有转移学习任务的实验，并对照简单的CNN和DAN基准对他们的模型进行了基准测试。该方法后来<a href="https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html" target="_blank" rel="noopener">扩展为解决多语言设置</a>。</p>
<p>的<em>变压器</em>模型直接基于在[提出的变压器模型<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">瓦斯瓦尼等人2017</a> ]，所述第一序列转导模型完全基于注意，取代在编码器-解码器的体系结构最常用的多双头自关注的复发性层（参见图22a）。</p>
<p>该模型使用转换器体系结构的编码子图构造句子嵌入。编码器使用注意力来计算句子中单词的上下文感知表示，同时考虑其他单词的顺序和身份。将上下文感知的单词表示形式平均在一起，以获得句子级的嵌入。</p>
<p>  <img src="20.png" alt="png"></p>
<p>图22：通用句子编码器的两种模型：（a）变压器和（b）DAN</p>
<p>相反，在[ <a href="https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf" target="_blank" rel="noopener">Iyyer et al，2015</a> ]中提出的DAN模型中，单词和<a href="https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf" target="_blank" rel="noopener">二元语法的</a>输入嵌入首先被平均在一起，然后通过前馈深度神经网络（DNN）生成句子嵌入（见图22b）。 。</p>
<p><strong>GenSen</strong><br>与通用句子编码器非常相似，[ <a href="https://arxiv.org/pdf/1804.00079.pdf" target="_blank" rel="noopener">Subramanian et al，2018</a> ]中介绍的GenSen方法与<a href="https://github.com/Maluuba/gensen" target="_blank" rel="noopener">官方Python实现一起</a>，结合了多个有监督和无监督的学习任务，以训练基于RNN w / GRU的编码器-解码器模型，嵌入被提取。支持的四个任务是：（1）<em>跳跳</em>向量，（2）神经机器翻译，（3）选区解析和（4）自然语言推论（三向分类问题；给定前提和假设句子，目的是将他们的关系归类为牵连，矛盾或中立。<a href="https://github.com/Maluuba/gensen" target="_blank" rel="noopener">正式的Python实现已发布</a>。</p>
<h2 id="如何选择各种嵌入方法"><a href="#如何选择各种嵌入方法" class="headerlink" title="如何选择各种嵌入方法"></a>如何选择各种嵌入方法</h2><p>我在这里没有简单的答案，但是这里有一些可能的要点：</p>
<ol>
<li><strong>平均单词向量是一个很强的基准</strong>，因此一个好主意是通过着重于生成非常好的单词向量并首先简单地对其求<strong>平均</strong>来开始寻求好的文档嵌入。毫无疑问，文档嵌入的强大功能大部分来自构建它们的词向量，我可以肯定地说，在前进之前，有大量信息要优化。您可以尝试不同的预训练词嵌入，探索哪些源域和哪些方法（例如word2vec，GloVe，BERT，ELMo）可以更好地捕获所需的信息类型。然后，通过尝试使用不同的汇总运算符或其他技巧（如[ <a href="https://pdfs.semanticscholar.org/3fc9/7768dc0b36449ec377d6a4cad8827908d5b4.pdf" target="_blank" rel="noopener">Arora等，2016</a> ]中的那些技巧）稍微扩展一下可能已足够。</li>
<li><strong>性能可能是一个关键考虑因素</strong>，尤其是在方法之间没有明确领导者的情况下。在这种情况下，<a href="https://translate.googleusercontent.com/translate_f#ecd3" target="_blank" rel="noopener">平均单词向量</a>和一些精益方法（如<a href="https://translate.googleusercontent.com/translate_f#e3d4" target="_blank" rel="noopener"><em>send2vec</em></a>和<a href="https://translate.googleusercontent.com/translate_f#e6e8" target="_blank" rel="noopener"><em>FastSent</em></a>）都是不错的选择。相反，在使用<em>doc2vec</em> 时，给定每个句子所需的实时向量表示推断，可能会因应用程序限制而证明代价高昂。<a href="https://github.com/facebookresearch/SentEval" target="_blank" rel="noopener">SentEval</a>是[ <a href="https://arxiv.org/pdf/1803.05449.pdf" target="_blank" rel="noopener">Conneau＆Kiela，2018</a> ]中提出的<a href="https://github.com/facebookresearch/SentEval" target="_blank" rel="noopener">用于句子表示的评估工具包，</a>在这种情况下值得一提。</li>
<li><strong>考虑学习目标对您任务的有效性</strong>。上面介绍的不同的自我监督技术以不同的方式扩展<em>了分布假设</em>，而<em>Skip-thought</em>和<em>快速思想则</em>基于句子/段落在文档中的距离对它们之间的紧密关系进行建模。这可能对书籍，文章和社交媒体帖子而言微不足道，但可能不适用于其他文本序列，尤其是结构化文本序列，因此可能会将您的文档投影到不适用于它们的嵌入空间中。同样，WME依赖的单词对齐方法可能并不适用于所有情况。</li>
<li><strong>开源实现非常丰富</strong>，因此针对您的任务对不同方法进行基准测试可能是可行的。</li>
<li><strong>没有明确的特定任务负责人。</strong>论文经常针对分类，释义和语义相关性任务对不同的方法进行基准测试。然而，上述结论是在考虑有关该主题的全部文献时，特别是在考虑了2018年的两个最新基准的结果时得出的，[ <a href="https://arxiv.org/pdf/1803.02893.pdf" target="_blank" rel="noopener">Logeswaran＆Lee，2018</a> ]首先提出了他们的<em>快速思考方法。</em>第二种方法是[ <a href="https://arxiv.org/pdf/1811.01713v1.pdf" target="_blank" rel="noopener">Wu et al，2018b</a> ]，这是他们关于<em>Word Mover嵌入</em>的论文的一部分。</li>
</ol>
<p>最后，我发现它值得一提的是<em>关键词中包含的代码</em>有<a href="https://paperswithcode.com/task/document-embedding" target="_blank" rel="noopener">一个专门的文件嵌入任务</a>，而且Facebook的研究具有开源<a href="https://github.com/facebookresearch/SentEval" target="_blank" rel="noopener">SentEval，评估工具包句子表示</a>在[呈现<a href="https://arxiv.org/pdf/1803.05449.pdf" target="_blank" rel="noopener">Conneau＆Kiela，2018</a> ]</p>
<p>本文章翻译自原文[原文连接]<a href="https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d.其对目前大概的文本嵌入方式进行了总结" target="_blank" rel="noopener">https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d.其对目前大概的文本嵌入方式进行了总结</a>. </p>
<p>我后面将会把最新的文本嵌入方法进行更新.并且计划将上述文本表示进行实践.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>Agibetov, A., Blagec, K., Xu, H., &amp; Samwald, M. (2018). <a href="https://link.springer.com/article/10.1186/s12859-018-2496-4" target="_blank" rel="noopener">Fast and scalable neural embedding models for biomedical sentence classification</a>. <em>BMC bioinformatics</em>, <em>19</em>(1), 541.</p>
<p>Ahmad, W. U., Bai, X., Peng, N., &amp; Chang, K. W. (2018). <a href="https://arxiv.org/pdf/1810.00681v1.pdf" target="_blank" rel="noopener">Learning Robust, Transferable Sentence Representations for Text Classification</a>. <em>arXiv preprint arXiv:1810.00681</em>.</p>
<p>Arora, S., Liang, Y., &amp; Ma, T. (2016). <a href="https://pdfs.semanticscholar.org/3fc9/7768dc0b36449ec377d6a4cad8827908d5b4.pdf" target="_blank" rel="noopener">A simple but tough-to-beat baseline for sentence embeddings</a>. [<a href="https://github.com/peter3125/sentence2vec" target="_blank" rel="noopener">unofficial implementation</a>]</p>
<p>Bengio, Y., Ducharme, R., Vincent, P., &amp; Jauvin, C. (2003). <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="noopener">A neural probabilistic language model</a>. <em>Journal of machine learning research</em>, <em>3</em>(Feb), 1137–1155.</p>
<p>B. Broere, (2017). <a href="http://arno.uvt.nl/show.cgi?fid=146003" target="_blank" rel="noopener">Syntactic properties of skip-thought vectors</a>. <em>Master’s thesis, Tilburg University</em>.</p>
<p>Cer, D., Yang, Y., Kong, S. Y., Hua, N., Limtiaco, N., John, R. S., … &amp; Sung, Y. H. (2018). <a href="https://arxiv.org/pdf/1803.11175.pdf" target="_blank" rel="noopener">Universal sentence encoder</a>. <em>arXiv preprint arXiv:1803.11175</em>.</p>
<p>Cer, D., Yang, Y., Kong, S. Y., Hua, N., Limtiaco, N., John, R. S., … &amp; Strope, B. (2018, November). <a href="https://www.aclweb.org/anthology/D18-2029/" target="_blank" rel="noopener">Universal sentence encoder for English</a>. In <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em> (pp. 169–174).</p>
<p>Chen, M. (2017). <a href="https://arxiv.org/pdf/1707.02377.pdf" target="_blank" rel="noopener">Efficient vector representation for documents through corruption</a>. <em>arXiv preprint arXiv:1707.02377</em>.</p>
<p>Chen, Q., Peng, Y., &amp; Lu, Z. (2018). <a href="https://arxiv.org/pdf/1810.09302.pdf" target="_blank" rel="noopener">BioSentVec: creating sentence embeddings for biomedical texts</a>. arXiv preprint arXiv:1810.09302.</p>
<p>Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). <a href="https://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">Learning phrase representations using RNN encoder-decoder for statistical machine translation</a>. <em>arXiv preprint arXiv:1406.1078</em>.</p>
<p>Cho, K., Van Merriënboer, B., Bahdanau, D., &amp; Bengio, Y. (2014). <a href="https://arxiv.org/abs/1409.1259" target="_blank" rel="noopener">On the properties of neural machine translation: Encoder-decoder approaches</a>. <em>arXiv preprint arXiv:1409.1259</em>.</p>
<p>Conneau, A., Kiela, D., Schwenk, H., Barrault, L., &amp; Bordes, A. (2017). <a href="https://www.aclweb.org/anthology/D17-1070.pdf" target="_blank" rel="noopener">Supervised learning of universal sentence representations from natural language inference data</a>. <em>arXiv preprint arXiv:1705.02364</em>.</p>
<p>Conneau, A., &amp; Kiela, D. (2018). <a href="https://arxiv.org/pdf/1803.05449.pdf" target="_blank" rel="noopener">Senteval: An evaluation toolkit for universal sentence representations</a>. <em>arXiv preprint arXiv:1803.05449</em>.</p>
<p>Dai, A. M., Olah, C., &amp; Le, Q. V. (2015). <a href="https://arxiv.org/pdf/1507.07998.pdf" target="_blank" rel="noopener">Document embedding with paragraph vectors</a>. <em>arXiv preprint arXiv:1507.07998</em>.</p>
<p>Das, A., Yenala, H., Chinnakotla, M., &amp; Shrivastava, M. (2016, August). <a href="https://www.aclweb.org/anthology/P16-1036" target="_blank" rel="noopener">Together we stand: Siamese networks for similar question retrieval</a>. In <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em> (pp. 378–387).</p>
<p>Gan, Z., Pu, Y., Henao, R., Li, C., He, X., &amp; Carin, L. (2016). Unsupervised learning of sentence representations using convolutional neural networks. <em>arXiv preprint arXiv:1611.07897</em>.</p>
<p>Gan, Z., Pu, Y., Henao, R., Li, C., He, X., &amp; Carin, L. (2016). <a href="https://arxiv.org/pdf/1611.07897.pdf" target="_blank" rel="noopener">Learning generic sentence representations using convolutional neural networks</a>. <em>arXiv preprint arXiv:1611.07897</em>.</p>
<p>Gupta, P., Pagliardini, M., &amp; Jaggi, M. (2019). <a href="https://www.aclweb.org/anthology/N19-1098" target="_blank" rel="noopener">Better Word Embeddings by Disentangling Contextual n-Gram Information</a>. <em>arXiv preprint arXiv:1904.05033</em>.</p>
<p>Harris, Z. S. (1954). Distributional structure. Word, 10(2–3), 146–162.</p>
<p>Hill, F., Cho, K., Korhonen, A., &amp; Bengio, Y. (2015). <a href="https://arxiv.org/pdf/1504.00548.pdf" target="_blank" rel="noopener">Learning to understand phrases by embedding the dictionary</a>. <em>Transactions of the Association for Computational Linguistics</em>, <em>4</em>, 17–30.</p>
<p>Hill, F., Cho, K., &amp; Korhonen, A. (2016). <a href="https://www.aclweb.org/anthology/N16-1162" target="_blank" rel="noopener">Learning distributed representations of sentences from unlabelled data</a>. <em>arXiv preprint arXiv:1602.03483</em>.</p>
<p>Huang, P. S., He, X., Gao, J., Deng, L., Acero, A., &amp; Heck, L. (2013, October). <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf" target="_blank" rel="noopener">Learning deep structured semantic models for web search using clickthrough data</a>. In <em>Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</em> (pp. 2333–2338). ACM.</p>
<p>Iyyer, M., Manjunatha, V., Boyd-Graber, J., &amp; Daumé III, H. (2015). <a href="https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf" target="_blank" rel="noopener">Deep unordered composition rivals syntactic methods for text classification</a>. In <em>Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em> (Vol. 1, pp. 1681–1691).</p>
<p>Josifoski, M., Paskov, I. S., Paskov, H. S., Jaggi, M., &amp; West, R. (2019, January). <a href="https://dl.acm.org/citation.cfm?id=3291023" target="_blank" rel="noopener">Crosslingual Document Embedding as Reduced-Rank Ridge Regression</a>. In <em>Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</em> (pp. 744–752). ACM.</p>
<p>Kalchbrenner, N., Grefenstette, E., &amp; Blunsom, P. (2014). A convolutional neural network for modelling sentences. <em>arXiv preprint arXiv:1404.2188</em>.</p>
<p>Kenter, T., Borisov, A., &amp; De Rijke, M. (2016). <a href="https://arxiv.org/pdf/1606.04640.pdf" target="_blank" rel="noopener">Siamese cbow: Optimizing word embeddings for sentence representations</a>. <em>arXiv preprint arXiv:1606.04640</em>.</p>
<p>Kim, Yoon. “Convolutional neural networks for sentence classification.” <em>arXiv preprint arXiv:1408.5882</em> (2014).</p>
<p>Kiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun, R., Torralba, A., &amp; Fidler, S. (2015). <a href="https://arxiv.org/abs/1506.06726" target="_blank" rel="noopener">Skip-thought vectors</a>. In <em>Advances in neural information processing systems</em> (pp. 3294–3302).</p>
<p>Kusner, M., Sun, Y., Kolkin, N., &amp; Weinberger, K. (2015, June). <a href="http://proceedings.mlr.press/v37/kusnerb15.pdf" target="_blank" rel="noopener">From word embeddings to document distances</a>. In <em>International conference on machine learning</em> (pp. 957–966).</p>
<p>Lau, J. H., &amp; Baldwin, T. (2016). <a href="https://arxiv.org/pdf/1607.05368.pdf" target="_blank" rel="noopener">An empirical evaluation of doc2vec with practical insights into document embedding generation</a>. <em>arXiv preprint arXiv:1607.05368</em>. [<a href="https://github.com/jhlau/doc2vec" target="_blank" rel="noopener">code</a>]</p>
<p>Le, Q., &amp; Mikolov, T. (2014, January). <a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" target="_blank" rel="noopener">Distributed representations of sentences and documents</a>. In <em>International conference on machine learning</em> (pp. 1188–1196).</p>
<p>Lee, T., &amp; Park, Y. (2018). <a href="https://openreview.net/forum?id=H1a37GWCZ" target="_blank" rel="noopener">UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT</a>.</p>
<p>Logeswaran, L., &amp; Lee, H. (2018). <a href="https://arxiv.org/pdf/1803.02893.pdf" target="_blank" rel="noopener">An efficient framework for learning sentence representations</a>. arXiv preprint arXiv:1803.02893.</p>
<p>Li, B., Liu, T., Du, X., Zhang, D., &amp; Zhao, Z. (2015). <a href="https://arxiv.org/abs/1512.08183" target="_blank" rel="noopener">Learning document embeddings by predicting n-grams for sentiment classification of long movie reviews</a>. <em>arXiv preprint arXiv:1512.08183</em>.</p>
<p>Liu, Y., &amp; Lapata, M. (2018). Learning structured text representations. Transactions of the Association for Computational Linguistics, 6, 63–75.</p>
<p>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). <a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Efficient estimation of word representations in vector space</a>. arXiv preprint arXiv:1301.3781.</p>
<p>Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">Distributed representations of words and phrases and their compositionality</a>. In <em>Advances in neural information processing systems</em> (pp. 3111–3119).</p>
<p>Nicosia, M., &amp; Moschitti, A. (2017, August). <a href="https://www.aclweb.org/anthology/K17-1027" target="_blank" rel="noopener">Learning contextual embeddings for structural semantic similarity using categorical information</a>. In <em>Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</em>(pp. 260–270).</p>
<p>Pagliardini, M., Gupta, P., &amp; Jaggi, M. (2017). <a href="https://aclweb.org/anthology/N18-1049" target="_blank" rel="noopener">Unsupervised learning of sentence embeddings using compositional n-gram features</a>. <em>arXiv preprint arXiv:1703.02507</em>.</p>
<p>Pennington, J., Socher, R., &amp; Manning, C. (2014, October). <a href="https://www.aclweb.org/anthology/D14-1162" target="_blank" rel="noopener">Glove: Global vectors for word representation</a>. In <em>Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</em> (pp. 1532–1543).</p>
<p>Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener"><em>Improving language understanding with unsupervised learning</em></a>. Technical report, OpenAI.</p>
<p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. (2019). <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener">Language models are unsupervised multitask learners</a>. <em>OpenAI Blog</em>, <em>1</em>(8).</p>
<p>Reimers, N., &amp; Gurevych, I. (2019). <a href="https://arxiv.org/pdf/1908.10084.pdf" target="_blank" rel="noopener">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a>. <em>arXiv preprint arXiv:1908.10084</em>.</p>
<p>Rudolph, M., Ruiz, F., Athey, S., &amp; Blei, D. (2017). Structured embedding models for grouped data. In <em>Advances in neural information processing systems</em> (pp. 251–261).</p>
<p>Salton, G., &amp; Buckley, C. (1988). <a href="http://pmcnamee.net/744/papers/SaltonBuckley.pdf" target="_blank" rel="noopener">Term-weighting approaches in automatic text retrieval</a>. <em>Information processing &amp; management</em>, <em>24</em>(5), 513–523.</p>
<p>Sinoara, R. A., Camacho-Collados, J., Rossi, R. G., Navigli, R., &amp; Rezende, S. O. (2019). <a href="https://www.sciencedirect.com/science/article/pii/S0950705118305124" target="_blank" rel="noopener">Knowledge-enhanced document embeddings for text classification</a>. <em>Knowledge-Based Systems</em>, <em>163</em>, 955–971.</p>
<p>Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., &amp; Potts, C. (2013, October). <a href="https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf" target="_blank" rel="noopener">Recursive deep models for semantic compositionality over a sentiment treebank</a>. In <em>Proceedings of the 2013 conference on empirical methods in natural language processing</em> (pp. 1631–1642).</p>
<p>Subramanian, S., Trischler, A., Bengio, Y., &amp; Pal, C. J. (2018). <a href="https://arxiv.org/pdf/1804.00079.pdf" target="_blank" rel="noopener">Learning general purpose distributed sentence representations via large scale multi-task learning</a>. <em>arXiv preprint arXiv:1804.00079</em>.</p>
<p>Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014). <a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">Sequence to sequence learning with neural networks</a>. In <em>Advances in neural information processing systems</em> (pp. 3104–3112).</p>
<p>Tang, S., Jin, H., Fang, C., Wang, Z., &amp; de Sa, V. R. (2017). <a href="https://arxiv.org/abs/1706.03146" target="_blank" rel="noopener">Rethinking skip-thought: A neighborhood based approach</a>. <em>arXiv preprint arXiv:1706.03146</em>.</p>
<p>Tang, S., Jin, H., Fang, C., Wang, Z., &amp; de Sa, V. R. (2017). <a href="https://www.groundai.com/project/trimming-and-improving-skip-thought-vectors/1" target="_blank" rel="noopener">Trimming and improving skip-thought vectors</a>. <em>arXiv preprint arXiv:1706.03148</em>.</p>
<p>Thongtan, T., &amp; Phienthrakul, T. (2019, July). Sentiment Classification using Document Embeddings trained with Cosine Similarity. In <em>Proceedings of the 57th Conference of the Association for Computational Linguistics: Student Research Workshop</em> (pp. 407–414).</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention is all you need</a>. <em>In Advances in neural information processing systems</em> (pp. 5998–6008).</p>
<p>Wieting, J., Bansal, M., Gimpel, K., &amp; Livescu, K. (2015). <a href="https://arxiv.org/pdf/1511.08198.pdf" target="_blank" rel="noopener">Towards universal paraphrastic sentence embeddings</a>. arXiv preprint arXiv:1511.08198.</p>
<p>Wieting, J., &amp; Gimpel, K. (2017). Revisiting recurrent networks for paraphrastic sentence embeddings. <em>arXiv preprint arXiv:1705.00364</em>.</p>
<p>Wu, L., Yen, I. E. H., Xu, F., Ravikumar, P., &amp; Witbrock, M. (2018). <a href="https://arxiv.org/pdf/1802.04956.pdf" target="_blank" rel="noopener">D2ke: From distance to kernel and embedding</a>. <em>arXiv preprint arXiv:1802.04956</em>.</p>
<p>Wu, L., Yen, I. E., Xu, K., Xu, F., Balakrishnan, A., Chen, P. Y., … &amp; Witbrock, M. J. (2018). <a href="https://arxiv.org/pdf/1811.01713v1.pdf" target="_blank" rel="noopener">Word Mover’s Embedding: From Word2Vec to Document Embedding</a>. <em>arXiv preprint arXiv:1811.01713</em>.</p>
<p>Yu, J., &amp; Jiang, J. (2016, November). <a href="https://www.semanticscholar.org/paper/Learning-Sentence-Embeddings-with-Auxiliary-Tasks-Yu-Jiang/2d38f7aab07d4435b2110602db4138ef20da4cc0" target="_blank" rel="noopener">Learning sentence embeddings with auxiliary tasks for cross-domain sentiment classification</a>. In <em>Proceedings of the 2016 conference on empirical methods in natural language processing</em> (pp. 236–246).</p>
<p>Zhang, Y., Chen, Q., Yang, Z., Lin, H., &amp; Lu, Z. (2019). BioWordVec, improving biomedical word embeddings with subword information and MeSH. <em>Scientific data</em>, <em>6</em>(1), 52.</p>
<p>Zhao, H., Lu, Z., &amp; Poupart, P. (2015, June). Self-adaptive hierarchical sentence model. In <em>Twenty-Fourth International Joint Conference on Artificial Intelligence</em>.</p>

    </div>

    
    
    
        
      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2020/08/01/docker/" rel="next" title="Docker">
                  <i class="fa fa-chevron-left"></i> Docker
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2020/08/13/nslookup/" rel="prev" title="Domain name system">
                  Domain name system <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/echarts.png"
      alt="Les">
  <p class="site-author-name" itemprop="name">Les</p>
  <div class="site-description motion-element" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">91</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">categories</span>
        
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span>
        
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>



        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#文件嵌入的应用"><span class="nav-number">1.</span> <span class="nav-text">文件嵌入的应用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#突出的方法和趋势"><span class="nav-number">2.</span> <span class="nav-text">突出的方法和趋势</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#文本嵌入的方法"><span class="nav-number">2.1.</span> <span class="nav-text">文本嵌入的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#挑战和趋势"><span class="nav-number">2.2.</span> <span class="nav-text">挑战和趋势</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#传统经典方法"><span class="nav-number">3.</span> <span class="nav-text">传统经典方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Bag-of-words"><span class="nav-number">3.1.</span> <span class="nav-text">Bag-of-words</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Latent-Dirichlet-allocation-LDA"><span class="nav-number">3.2.</span> <span class="nav-text">Latent Dirichlet allocation (LDA)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#无监督的文本嵌入"><span class="nav-number">4.</span> <span class="nav-text">无监督的文本嵌入</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#n-gram-embeddings"><span class="nav-number">4.1.</span> <span class="nav-text">n-gram embeddings</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Averaging-word-embeddings"><span class="nav-number">4.2.</span> <span class="nav-text">Averaging word embeddings</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sent2Vec"><span class="nav-number">4.3.</span> <span class="nav-text">Sent2Vec</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Paragraph-vectors-doc2vec"><span class="nav-number">4.4.</span> <span class="nav-text">Paragraph vectors (doc2vec)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Paragraph-Vectors-Distributed-Memory-PV-DM"><span class="nav-number">4.5.</span> <span class="nav-text">Paragraph Vectors: Distributed Memory (PV-DM)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Doc2VecC"><span class="nav-number">4.6.</span> <span class="nav-text">Doc2VecC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Skip-thought-vectors"><span class="nav-number">4.7.</span> <span class="nav-text">Skip-thought vectors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FastSent"><span class="nav-number">4.8.</span> <span class="nav-text">FastSent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Quick-thought-vectors"><span class="nav-number">4.9.</span> <span class="nav-text">Quick-thought vectors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Word-Mover’s-Embedding-WME"><span class="nav-number">4.10.</span> <span class="nav-text">Word Mover’s Embedding (WME)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sentence-BERT-SBERT"><span class="nav-number">4.11.</span> <span class="nav-text">Sentence-BERT (SBERT)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#有监督的向量嵌入"><span class="nav-number">5.</span> <span class="nav-text">有监督的向量嵌入</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#文档相似性的上下文嵌入"><span class="nav-number">5.1.</span> <span class="nav-text">文档相似性的上下文嵌入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特定于任务的监督文档嵌入"><span class="nav-number">5.2.</span> <span class="nav-text">特定于任务的监督文档嵌入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#联合学习句子表示"><span class="nav-number">5.3.</span> <span class="nav-text">联合学习句子表示</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#如何选择各种嵌入方法"><span class="nav-number">6.</span> <span class="nav-text">如何选择各种嵌入方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">7.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Les</span>
</div>

        








        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
      </div>

    

  </div>

  
    
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

<script src="/js/utils.js?v=7.3.0"></script>
  <script src="/js/motion.js?v=7.3.0"></script>


  <script src="/js/affix.js?v=7.3.0"></script>
  <script src="/js/schemes/pisces.js?v=7.3.0"></script>


<script src="/js/next-boot.js?v=7.3.0"></script>




  




























  

  
    
      <script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', function() {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


  
  <script src="/js/scrollspy.js?v=7.3.0"></script>
<script src="/js/post-details.js?v=7.3.0"></script>


</body>
</html>
